{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8mEilfefYqb4",
      "metadata": {
        "id": "8mEilfefYqb4"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "qPTWSFu-Yqb6",
      "metadata": {
        "id": "qPTWSFu-Yqb6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import albumentations as A # fast image agumentation library\n",
        "from albumentations.pytorch.transforms import ToTensorV2 # 이미지 형 변환\n",
        "#import torchvision.models as models\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e83efdc4",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "NjkSKcj3Yqb8",
      "metadata": {
        "id": "NjkSKcj3Yqb8"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# gpu 사용하기 위한 코드\n",
        "# cuda가 설치되어 있으면 gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IKfY4MLYYqb8",
      "metadata": {
        "id": "IKfY4MLYYqb8"
      },
      "source": [
        "## Hyperparameter Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "Ag8ewGK5Yqb9",
      "metadata": {
        "id": "Ag8ewGK5Yqb9"
      },
      "outputs": [],
      "source": [
        "CFG = {\n",
        "    'IMG_SIZE':224,\n",
        "    'EPOCHS':20,\n",
        "    'LEARNING_RATE':3e-4,\n",
        "    'BATCH_SIZE':16,\n",
        "    'SEED':41\n",
        "}\n",
        "# 이미지 사이즈, 이폭, 학습률, 배치사이즈, 시드 고정"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NHVDirSbYqb-",
      "metadata": {
        "id": "NHVDirSbYqb-"
      },
      "source": [
        "## Fixed RandomSeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "uDCxH4oxYqb-",
      "metadata": {
        "id": "uDCxH4oxYqb-"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(CFG['SEED']) # Seed 고정"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o7K4p5NkYqb_",
      "metadata": {
        "id": "o7K4p5NkYqb_"
      },
      "source": [
        "## Data Load & Train/Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "OQyg4_lJYqcA",
      "metadata": {
        "id": "OQyg4_lJYqcA"
      },
      "outputs": [],
      "source": [
        "all_df = pd.read_csv('./train_bert.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "EcTB0OXYYqcB",
      "metadata": {
        "id": "EcTB0OXYYqcB"
      },
      "outputs": [],
      "source": [
        "train_df, val_df, _, _ = train_test_split(all_df, all_df['cat3'], test_size=0.2, random_state=CFG['SEED'])\n",
        "# train set, validation set 구별"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3SepiZ4ZYqcB",
      "metadata": {
        "id": "3SepiZ4ZYqcB"
      },
      "source": [
        "## Label-Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "D7cJtPy8YqcB",
      "metadata": {
        "id": "D7cJtPy8YqcB",
        "outputId": "bcfb2c88-b28a-4083-b90d-6421238dc70b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(train_df['cat3'].values)\n",
        "# 카테고리형 데이터를 수치형으로 변환하는 labelencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "DWmb25r_YqcC",
      "metadata": {
        "id": "DWmb25r_YqcC"
      },
      "outputs": [],
      "source": [
        "train_df['cat3'] = le.transform(train_df['cat3'].values)\n",
        "val_df['cat3'] = le.transform(val_df['cat3'].values)\n",
        "# cat3에 labelencoder를 적용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OkIwC6lBYqcD",
      "metadata": {
        "id": "OkIwC6lBYqcD"
      },
      "source": [
        "## Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "RTlinSBbYqcD",
      "metadata": {
        "id": "RTlinSBbYqcD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from transformers import BertTokenizerFast, BertModel\n",
        "# tokenizer_bert = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
        "# model_bert = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "FgdPdmIbYqcD",
      "metadata": {
        "id": "FgdPdmIbYqcD"
      },
      "outputs": [],
      "source": [
        "# vectorizer = CountVectorizer(max_features=4096)\n",
        "# # overview를 vectorize하는 vectorizer 선언, 최대 특성 수는 4096\n",
        "\n",
        "# train_vectors = vectorizer.fit_transform(train_df['overview'])\n",
        "# train_vectors = train_vectors.todense()\n",
        "\n",
        "# val_vectors = vectorizer.transform(val_df['overview'])\n",
        "# val_vectors = val_vectors.todense()\n",
        "\n",
        "# train_vectors = vectorizer.fit_transform(train_df['overview'])\n",
        "# train_vectors = train_vectors.todense()\n",
        "\n",
        "# val_vectors = vectorizer.transform(val_df['overview'])\n",
        "# val_vectors = val_vectors.todense()\n",
        "train_vectors = []\n",
        "for i, emb in enumerate(train_df['bert_emb']):\n",
        "    if isinstance(emb, str):\n",
        "        arr = emb.replace('[', '').replace(']', '').split()\n",
        "        float_arr = [float(x) for x in arr]\n",
        "        train_vectors.append(float_arr)\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "val_vectors = []       \n",
        "for i, emb in enumerate(val_df['bert_emb']):\n",
        "    if isinstance(emb, str):\n",
        "        arr = emb.replace('[', '').replace(']', '').split()\n",
        "        float_arr = [float(x) for x in arr]\n",
        "        val_vectors.append(float_arr)\n",
        "    else:\n",
        "        continue\n",
        "    \n",
        "train_vectors = np.array(train_vectors, dtype=np.float32)\n",
        "val_vectors = np.array(val_vectors, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "108d56ae",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0mSQ4CZrYqcE",
      "metadata": {
        "id": "0mSQ4CZrYqcE",
        "outputId": "eb1e1b79-ab3c-4790-e8d4-76f7df521661"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "train_vectors.shape\n",
        "for a in train_vectors:\n",
        "    if isinstance(a, np.ndarray):\n",
        "        pass\n",
        "    else:\n",
        "        print(type(a))\n",
        "        print(a)\n",
        "print(type(train_vectors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "GWoFexU4YqcE",
      "metadata": {
        "id": "GWoFexU4YqcE",
        "outputId": "82a333f9-1c1a-4759-c917-331b0e4d6f41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3398, 768)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_vectors.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40c3607d",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "P3xNr-gCYqcE",
      "metadata": {
        "id": "P3xNr-gCYqcE"
      },
      "source": [
        "## CustomDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "gWjyLPMDYqcE",
      "metadata": {
        "id": "gWjyLPMDYqcE"
      },
      "outputs": [],
      "source": [
        "# Dataset 생성\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, img_path_list, text_vectors, label_list, transforms, infer=False):\n",
        "        self.img_path_list = img_path_list\n",
        "        self.text_vectors = text_vectors\n",
        "        self.label_list = label_list\n",
        "        self.transforms = transforms\n",
        "        self.infer = infer\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # NL\n",
        "        text_vector = self.text_vectors[index]\n",
        "\n",
        "        # Image 읽기\n",
        "        img_path = self.img_path_list[index]\n",
        "        #image = Image.open(img_path)\n",
        "        image = cv2.imread(img_path)\n",
        "        \n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image=image)['image'] # transforms(=image augmentation) 적용\n",
        "\n",
        "         # Label\n",
        "        if self.infer: # infer == True, test_data로부터 label \"결과 추출\" 시 사용\n",
        "            return image, text_vector\n",
        "        else: # infer == False\n",
        "            label = self.label_list[index] # dataframe에서 label 가져와 \"학습\" 시 사용\n",
        "            return image, text_vector, label\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_path_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "JrdLk7BvYqcF",
      "metadata": {
        "id": "JrdLk7BvYqcF"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "                            A.Resize(CFG['IMG_SIZE'],CFG['IMG_SIZE']),\n",
        "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
        "                            ToTensorV2()\n",
        "                            ])\n",
        "\n",
        "test_transform = A.Compose([\n",
        "                            A.Resize(CFG['IMG_SIZE'],CFG['IMG_SIZE']),\n",
        "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
        "                            ToTensorV2()\n",
        "                            ])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W6mw3pJXYqcF",
      "metadata": {
        "id": "W6mw3pJXYqcF"
      },
      "source": [
        "- albumentations -> fast image augmentation library\n",
        "\n",
        "- albumentations.Compose -> transform = A.Compose([])을 이용하여 이미지와 라벨 각각에 Augmentation을 적용하기 위한 객체를 생성\n",
        "\n",
        "- albumentations.Resize(128, 128) -> 128*128 size로 resize\n",
        "- albumentations.Normalize() -> 입력 받은 이미지 값의 범위를 (0, 255) → (-1, 1) 범위로 줄여주는 역할, 위에서는 평균값, 분산값, 최대 픽셀값으로 img = (img - mean * max_pixel_value) / (std * max_pixel_value)을 계산.\n",
        "- ToTensorV2 -> tensor형 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "28lLznspYqcF",
      "metadata": {
        "id": "28lLznspYqcF"
      },
      "outputs": [],
      "source": [
        "# __init__(self, img_path_list, text_vectors, label_list, transforms, infer=False)\n",
        "train_dataset = CustomDataset(train_df['img_path'].values, train_vectors, train_df['cat3'].values, train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0) # 6\n",
        "\n",
        "val_dataset = CustomDataset(val_df['img_path'].values, val_vectors, val_df['cat3'].values, test_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0) # 6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AjwCGRhnYqcG",
      "metadata": {
        "id": "AjwCGRhnYqcG"
      },
      "source": [
        "- DataLoader: Dataset와 Sampler를 결합하고 지정된 데이터 세트에 대해 반복 가능한 기능을 제공.    \n",
        "    - dataset (Dataset): 데이터를 로드할 데이터 집합.   \n",
        "    - batch_size (int, optional): **how many samples** per batch to load (default: ``1``).   \n",
        "    - num_workers (int, optional): **how many subprocesses** to use for data loading. ``0`` means that the data will be    loaded in the main process. (default: ``0``) -> 6으로 설정 시 안돌아감([Errno 32] Broken pipe). 0으로 변경해야 됨"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OOBrsD0nYqcG",
      "metadata": {
        "id": "OOBrsD0nYqcG"
      },
      "source": [
        "## Model Define"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "O_hoH59gYqcG",
      "metadata": {
        "id": "O_hoH59gYqcG"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, num_classes=len(le.classes_)):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.resnet = resnet50(weights=ResNet50_Weights.DEFAULT).to(device)\n",
        "        #self.effnet = EfficientNet.from_pretrained('efficientnet-b0')      \n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1000, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "\n",
        "        self.nlp_extract = nn.Sequential(\n",
        "            nn.Linear(768, 1000), # 선형회귀. 4096개의 입력으로 2048개의 출력\n",
        "        )\n",
        "\n",
        "    def forward(self, img, text):\n",
        "        img_feature = self.resnet(img)\n",
        "        text_feature = self.nlp_extract(text)\n",
        "\n",
        "        feature = img_feature + text_feature\n",
        "\n",
        "        #feature = torch.cat([img_feature, text], axis=1) # 2개 연결(1000 + 768)\n",
        "        output = self.classifier(feature) # classifier 적용\n",
        "        #self.softmax(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TajLAFGWYqcG",
      "metadata": {
        "id": "TajLAFGWYqcG"
      },
      "source": [
        "결론:\n",
        "- Image: conv -> ReLU -> MaxPooling -> conv -> relu -> maxpooling -> conv -> relu -> maxpooling -> conv -> relu -> maxpooling\n",
        "\n",
        "- Text: linear -> relu -> linear\n",
        "\n",
        "- classifier : linear"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9NWkAbI2YqcG",
      "metadata": {
        "id": "9NWkAbI2YqcG"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "Q4SIuzRRYqcG",
      "metadata": {
        "id": "Q4SIuzRRYqcG"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
        "    model.to(device) # gpu(cpu)에 적용\n",
        "    criterion = nn.CrossEntropyLoss().to(device) # CrossEntropyLoss: 다중분류를 위한 손실함수\n",
        "    best_score = 0\n",
        "    best_model = None # 최고의 모델을 추출하기 위한 파라미터\n",
        "    \n",
        "    for epoch in range(1,CFG[\"EPOCHS\"]+1):\n",
        "        model.train() # 학습시킴.\n",
        "        train_loss = []\n",
        "        correct = 0\n",
        "        for img, text, label in tqdm(iter(train_loader)): # train_loader에서 img, text, label 가져옴\n",
        "            img = img.float().to(device)\n",
        "            text = text.to(device)\n",
        "            label = label.type(torch.LongTensor) # label type을 LongTensor로 형변환, 추가하여 에러 해결\n",
        "            label = label.to(device)\n",
        "            \n",
        "            optimizer.zero_grad() # 이전 루프에서 .grad에 저장된 값이 다음 루프의 업데이트에도 간섭하는 걸 방지, 0으로 초기화\n",
        "\n",
        "            model_pred = model(img, text) # 예측\n",
        "            \n",
        "            loss = criterion(model_pred, label) # 예측값과 실제값과의 손실 계산\n",
        "\n",
        "            loss.backward() # .backward() 를 호출하면 역전파가 시작\n",
        "            optimizer.step() # optimizer.step()을 호출하여 역전파 단계에서 수집된 변화도로 매개변수를 조정\n",
        "\n",
        "            train_loss.append(loss.item())\n",
        "\n",
        "            pred_label = torch.argmax(model_pred, dim=1)\n",
        "            correct += (label == pred_label).float().sum()\n",
        "        \n",
        "        accuracy =  correct / float(len(train_loader.dataset))\n",
        "        # trainset, not train_loader\n",
        "        # probably x in your case\n",
        "\n",
        "        print(\"Train Accuracy = {}\".format(accuracy))\n",
        "            \n",
        "        # 모든 train_loss 가져옴\n",
        "        tr_loss = np.mean(train_loss)\n",
        "            \n",
        "        val_loss, val_score = validation(model, criterion, val_loader, device) # 검증 시작, 여기서 validation 함수 사용\n",
        "            \n",
        "        print(f'Epoch [{epoch}], Train Loss : [{tr_loss:.5f}] Val Loss : [{val_loss:.5f}] Val Score : [{val_score:.5f}]')\n",
        "        \n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "            # scheduler의 의미: Learning Rate Scheduler => learning rate를 조절한다. \n",
        "            # DACON에서는 CosineAnnealingLR 또는 CosineAnnealingWarmRestarts 를 주로 사용한다.\n",
        "            \n",
        "        if best_score < val_score: # 최고의 val_score을 가진 모델에 대해서만 최종적용을 시킴\n",
        "            best_score = val_score\n",
        "            best_model = model\n",
        "    \n",
        "    return best_model # val_score가 가장 높은 모델을 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ZDH3nVsTYqcH",
      "metadata": {
        "id": "ZDH3nVsTYqcH"
      },
      "outputs": [],
      "source": [
        "def score_function(real, pred):\n",
        "    return f1_score(real, pred, average=\"weighted\")\n",
        "\n",
        "def validation(model, criterion, val_loader, device):\n",
        "    model.eval() # nn.Module에서 train time과 eval time에서 수행하는 다른 작업을 수행할 수 있도록 switching 하는 함수\n",
        "    \n",
        "    model_preds = [] # 예측값\n",
        "    true_labels = [] # 실제값\n",
        "    \n",
        "    val_loss = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for img, text, label in tqdm(iter(val_loader)): # val_loader에서 img, text, label 가져옴\n",
        "            img = img.float().to(device)\n",
        "            text = text.to(device)\n",
        "            label = label.type(torch.LongTensor) # label type을 LongTensor로 형변환, 추가하여 에러 해결\n",
        "            label = label.to(device)\n",
        "            \n",
        "            model_pred = model(img, text)\n",
        "            \n",
        "            loss = criterion(model_pred, label) # 예측값, 실제값으로 손실함수 적용 -> loss 추출\n",
        "            \n",
        "            val_loss.append(loss.item()) # loss 출력, val_loss에 저장\n",
        "            \n",
        "            model_preds += model_pred.argmax(1).detach().cpu().numpy().tolist()\n",
        "            true_labels += label.detach().cpu().numpy().tolist()\n",
        "        \n",
        "    test_weighted_f1 = score_function(true_labels, model_preds) # 실제 라벨값들과 예측한 라벨값들에 대해 f1 점수 계산\n",
        "    return np.mean(val_loss), test_weighted_f1 # 각각 val_loss, val_score에 적용됨"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gStGzRq_YqcH",
      "metadata": {
        "id": "gStGzRq_YqcH"
      },
      "source": [
        "## Run!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "a9410c0f",
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 6.00 GiB total capacity; 5.27 GiB already allocated; 0 bytes free; 5.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCustomModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEARNING_RATE\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
            "Cell \u001b[1;32mIn [20], line 7\u001b[0m, in \u001b[0;36mCustomModel.__init__\u001b[1;34m(self, num_classes)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(le\u001b[38;5;241m.\u001b[39mclasses_)):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28msuper\u001b[39m(CustomModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnet \u001b[38;5;241m=\u001b[39m \u001b[43mresnet50\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mResNet50_Weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEFAULT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#self.effnet = EfficientNet.from_pretrained('efficientnet-b0')      \u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Classifier\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m     12\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m512\u001b[39m),\n\u001b[0;32m     13\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[0;32m     14\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m512\u001b[39m, num_classes),\n\u001b[0;32m     15\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    924\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m    925\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> 927\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\torch\\nn\\modules\\module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 602\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    603\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    604\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\torch\\nn\\modules\\module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m    923\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    924\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> 925\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 6.00 GiB total capacity; 5.27 GiB already allocated; 0 bytes free; 5.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "model = CustomModel()\n",
        "model.eval()\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
        "scheduler = None\n",
        "\n",
        "print(len(train_loader.dataset))\n",
        "infer_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f1ce547",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   iter    |  target   |   gamma   | max_depth | subsample |\n",
            "-------------------------------------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\bayes_opt\\target_space.py:191\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cache[_hashable(x)]\n\u001b[0;32m    192\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
            "\u001b[1;31mKeyError\u001b[0m: (5.4881350392732475, 2.430378732744839, 0.801381688035822)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [29], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m BO_xgb \u001b[38;5;241m=\u001b[39m BayesianOptimization(f \u001b[38;5;241m=\u001b[39m xgb_bo, pbounds \u001b[38;5;241m=\u001b[39m xgb_parameter_bounds,random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Bayesian Optimization을 실행해보세요\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mBO_xgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_points\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py:185\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    182\u001b[0m     x_probe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuggest(util)\n\u001b[0;32m    183\u001b[0m     iteration \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 185\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprobe(x_probe, lazy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    187\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bounds_transformer:\n\u001b[0;32m    188\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_bounds(\n\u001b[0;32m    189\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bounds_transformer\u001b[39m.\u001b[39mtransform(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_space))\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py:116\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_queue\u001b[39m.\u001b[39madd(params)\n\u001b[0;32m    115\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 116\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_space\u001b[39m.\u001b[39;49mprobe(params)\n\u001b[0;32m    117\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch(Events\u001b[39m.\u001b[39mOPTIMIZATION_STEP)\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\bayes_opt\\target_space.py:194\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_keys, x))\n\u001b[1;32m--> 194\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[0;32m    195\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister(x, target)\n\u001b[0;32m    196\u001b[0m \u001b[39mreturn\u001b[39;00m target\n",
            "Cell \u001b[1;32mIn [29], line 13\u001b[0m, in \u001b[0;36mxgb_bo\u001b[1;34m(gamma, max_depth, subsample)\u001b[0m\n\u001b[0;32m      5\u001b[0m xgb_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(gamma)),\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(max_depth)),\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubsample\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(subsample)),      \n\u001b[0;32m      9\u001b[0m }\n\u001b[0;32m     11\u001b[0m xgb_clf \u001b[38;5;241m=\u001b[39m XGBClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mxgb_params)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mxgb_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m score \u001b[38;5;241m=\u001b[39m accuracy_score(val_dataset\u001b[38;5;241m.\u001b[39mlabel_list, xgb_clf\u001b[38;5;241m.\u001b[39mpredict(val_dataset\u001b[38;5;241m.\u001b[39mtext_vectors))\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\xgboost\\core.py:575\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    574\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 575\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\xgboost\\sklearn.py:1400\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1379\u001b[0m model, metric, params, early_stopping_rounds, callbacks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_configure_fit(\n\u001b[0;32m   1380\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1381\u001b[0m )\n\u001b[0;32m   1382\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1383\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[0;32m   1384\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1397\u001b[0m     enable_categorical\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menable_categorical,\n\u001b[0;32m   1398\u001b[0m )\n\u001b[1;32m-> 1400\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1401\u001b[0m     params,\n\u001b[0;32m   1402\u001b[0m     train_dmatrix,\n\u001b[0;32m   1403\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[0;32m   1404\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m   1405\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m   1406\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m   1407\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m   1408\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m   1409\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1410\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1411\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1412\u001b[0m )\n\u001b[0;32m   1414\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[0;32m   1415\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\xgboost\\core.py:575\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    574\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 575\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[0;32m    182\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\xgboost\\core.py:1778\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_features(dtrain)\n\u001b[0;32m   1777\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1778\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   1779\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[0;32m   1780\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[0;32m   1781\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "def xgb_bo(gamma,max_depth, subsample):\n",
        "    xgb_params = {\n",
        "        'gamma' : int(round(gamma)),\n",
        "        'max_depth' : int(round(max_depth)),\n",
        "        'subsample' : int(round(subsample)),      \n",
        "    }\n",
        "\n",
        "    xgb_clf = XGBClassifier(**xgb_params)\n",
        "    \n",
        "    xgb_clf.fit(train_dataset.text_vectors,train_dataset.label_list)\n",
        "\n",
        "    score = accuracy_score(val_dataset.label_list, xgb_clf.predict(val_dataset.text_vectors))\n",
        "\n",
        "    return score\n",
        "\n",
        "xgb_parameter_bounds = {\n",
        "    'gamma' : (0,10),\n",
        "    'max_depth' : (1,3), # 나무의 깊이\n",
        "    'subsample' : (0.5,1)\n",
        "}\n",
        "\n",
        "BO_xgb = BayesianOptimization(f = xgb_bo, pbounds = xgb_parameter_bounds,random_state = 0)\n",
        "\n",
        "# Bayesian Optimization을 실행해보세요\n",
        "\n",
        "BO_xgb.maximize(init_points = 5, n_iter = 5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a82ab535",
      "metadata": {},
      "source": [
        "# XGBoost Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g6x70sUqYqcH",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "970fb5ea76dc4b78a957370c1c81ee77",
            "9d09a89302a44259878e43b7d779ebdf",
            "6f8cd79f16864f3785f762e66dcd56fd",
            "2d7beb20eab844f595265748e4263d5f",
            "85c6c544b4584dc1bb75e92df40cc225",
            "3a9dec68ddb04929829a929806dab517",
            "916b5d6ead744b3a98a9d1e9081e4ec6",
            "ec5f883fc96b4671bceba18aecb8999f",
            "4f9055798b1d44e28dac4136e4f6bdef",
            "9b3a8d415c9848b6b773fcdc85315ab8"
          ]
        },
        "id": "g6x70sUqYqcH",
        "outputId": "a5c7dbde-af4a-485e-a665-d30d15740ded",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Averaged train CV Accuracy: 0.99 (+/- 0.00)\n",
            "Averaged val CV Accuracy: 0.56 (+/- 0.00)\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "#BOW_XGB = xgb.XGBClassifier(learning_rate=0.1, max_depth=3, n_estimators=100, objective=\"multi:softprob\", random_state=1, tree_method='gpu_hist', predictor='gpu_predictor')\n",
        "BOW_XGB = xgb.XGBClassifier(max_depth=3, n_estimators=100, objective=\"multi:softprob\")\n",
        "BOW_XGB.fit(train_dataset.text_vectors, train_dataset.label_list)\n",
        "\n",
        "\n",
        "train_score = BOW_XGB.score(train_dataset.text_vectors, train_dataset.label_list)\n",
        "val_score = BOW_XGB.score(val_dataset.text_vectors, val_dataset.label_list)\n",
        "print(\"Averaged train CV Accuracy: %0.2f (+/- %0.2f)\" % (train_score.mean(), train_score.std() * 2))\n",
        "print(\"Averaged val CV Accuracy: %0.2f (+/- %0.2f)\" % (val_score.mean(), val_score.std() * 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d77e984",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0.00776025, 0.00775048, 0.00774799, 0.0078957 , 0.00774468,\n",
              "        0.00775019, 0.00774485, 0.00774694, 0.00774736, 0.00774509,\n",
              "        0.00774565, 0.00790768, 0.00774506, 0.0077469 , 0.00774525,\n",
              "        0.00774572, 0.00777256, 0.00775631, 0.00775758, 0.00775535,\n",
              "        0.00776521, 0.00805745, 0.00774643, 0.00774767, 0.007745  ,\n",
              "        0.0077451 , 0.0077516 , 0.00774597, 0.00774909, 0.00774504,\n",
              "        0.00774586, 0.007753  , 0.00774495, 0.00774577, 0.00774477,\n",
              "        0.00774454, 0.00774637, 0.00774577, 0.00774695, 0.00778864,\n",
              "        0.00774546, 0.00787829, 0.0077458 , 0.00774451, 0.00775682,\n",
              "        0.00774643, 0.00774597, 0.00774812, 0.00774426, 0.00774803,\n",
              "        0.00774612, 0.00774564, 0.00779119, 0.00776246, 0.00775944,\n",
              "        0.00775311, 0.00774866, 0.00774756, 0.00778362, 0.0077457 ,\n",
              "        0.00774733, 0.0077507 , 0.00774624, 0.00774527, 0.00774849,\n",
              "        0.00774641, 0.00774565, 0.00774784, 0.00774593, 0.00774507,\n",
              "        0.00774595, 0.00774807, 0.00774763, 0.00782807, 0.00774652,\n",
              "        0.00774572, 0.00774538, 0.0077503 , 0.0077456 , 0.00775008,\n",
              "        0.00774545, 0.00779168, 0.00781779, 0.00774492, 0.00785617,\n",
              "        0.00777992, 0.00821959, 0.00774544, 0.00774575, 0.00775056,\n",
              "        0.00774569, 0.00774667, 0.00774922, 0.0081004 , 0.00774558,\n",
              "        0.0077458 , 0.00774596, 0.00784884, 0.00774541, 0.00781318,\n",
              "        0.007747  , 0.00774413, 0.00774593, 0.00774515, 0.00774683,\n",
              "        0.0077456 , 0.00774509, 0.00774562, 0.00775349, 0.00774402,\n",
              "        0.00775675, 0.00776898, 0.00782126, 0.00774449, 0.00774492,\n",
              "        0.00774913, 0.00776987, 0.00774552, 0.00960345, 0.0077475 ,\n",
              "        0.01180149, 0.00777276, 0.00774687, 0.00774593, 0.00774533,\n",
              "        0.00776236, 0.00774801, 0.00775243], dtype=float32),\n",
              " 21)"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def softmax(x):\n",
        "    max = np.max(x,axis=1,keepdims=True) #returns max of each row and keeps same dims\n",
        "    e_x = np.exp(x - max) #subtracts each row with its max value\n",
        "    sum = np.sum(e_x,axis=1,keepdims=True) #returns sum of each row and keeps same dims\n",
        "    f_x = e_x / sum \n",
        "    return f_x\n",
        "\n",
        "result = BOW_XGB.predict_proba(val_dataset.text_vectors)\n",
        "result = softmax(result)\n",
        "result[0], val_dataset.label_list[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72891765",
      "metadata": {},
      "source": [
        "# NB Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4218260e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(train_dataset.text_vectors, train_dataset.label_list)\n",
        "\n",
        "train_score = nb_classifier.score(train_dataset.text_vectors, train_dataset.label_list)\n",
        "val_score = nb_classifier.score(val_dataset.text_vectors, val_dataset.label_list)\n",
        "\n",
        "print(\"Averaged train CV Accuracy: %0.2f (+/- %0.2f)\" % (train_score.mean(), train_score.std() * 2))\n",
        "print(\"Averaged val CV Accuracy: %0.2f (+/- %0.2f)\" % (val_score.mean(), val_score.std() * 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d66ab13",
      "metadata": {},
      "source": [
        "# RandomForest Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4ffb528",
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [28], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m      3\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m train_score \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mscore(train_dataset\u001b[38;5;241m.\u001b[39mtext_vectors, train_dataset\u001b[38;5;241m.\u001b[39mlabel_list)\n\u001b[0;32m      7\u001b[0m val_score \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mscore(val_dataset\u001b[38;5;241m.\u001b[39mtext_vectors, val_dataset\u001b[38;5;241m.\u001b[39mlabel_list)\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:476\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    465\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    466\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    467\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    468\u001b[0m ]\n\u001b[0;32m    470\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 476\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    477\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    478\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    479\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    480\u001b[0m )(\n\u001b[0;32m    481\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    482\u001b[0m         t,\n\u001b[0;32m    483\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    484\u001b[0m         X,\n\u001b[0;32m    485\u001b[0m         y,\n\u001b[0;32m    486\u001b[0m         sample_weight,\n\u001b[0;32m    487\u001b[0m         i,\n\u001b[0;32m    488\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    489\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    490\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    491\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    492\u001b[0m     )\n\u001b[0;32m    493\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    494\u001b[0m )\n\u001b[0;32m    496\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:189\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    187\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 189\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    190\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\sklearn\\tree\\_classes.py:969\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    940\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    970\u001b[0m         X,\n\u001b[0;32m    971\u001b[0m         y,\n\u001b[0;32m    972\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    973\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    974\u001b[0m     )\n\u001b[0;32m    975\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\sklearn\\tree\\_classes.py:458\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    448\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    449\u001b[0m         splitter,\n\u001b[0;32m    450\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    456\u001b[0m     )\n\u001b[1;32m--> 458\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    461\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=5,random_state=0)\n",
        "rf.fit(train_dataset.text_vectors, train_dataset.label_list)\n",
        "\n",
        "train_score = rf.score(train_dataset.text_vectors, train_dataset.label_list)\n",
        "val_score = rf.score(val_dataset.text_vectors, val_dataset.label_list)\n",
        "\n",
        "print(\"Averaged train CV Accuracy: %0.2f (+/- %0.2f)\" % (train_score.mean(), train_score.std() * 2))\n",
        "print(\"Averaged val CV Accuracy: %0.2f (+/- %0.2f)\" % (val_score.mean(), val_score.std() * 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c588d8a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Averaged train CV Accuracy: 0.36 (+/- 0.00)\n",
            "Averaged val CV Accuracy: 0.35 (+/- 0.00)\n"
          ]
        }
      ],
      "source": [
        "train_score = rf.score(train_dataset.text_vectors, train_dataset.label_list)\n",
        "val_score = rf.score(val_dataset.text_vectors, val_dataset.label_list)\n",
        "\n",
        "print(\"Averaged train CV Accuracy: %0.2f (+/- %0.2f)\" % (train_score.mean(), train_score.std() * 2))\n",
        "print(\"Averaged val CV Accuracy: %0.2f (+/- %0.2f)\" % (val_score.mean(), val_score.std() * 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30d7f019",
      "metadata": {},
      "source": [
        "# Load Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cce9f02",
      "metadata": {},
      "outputs": [],
      "source": [
        "# load test dataset\n",
        "test_df = pd.read_csv('./test.csv')\n",
        "test_overview = []\n",
        "for overview in test_df['overview']:\n",
        "    arr = mecab.nouns(overview)\n",
        "    noun_str = ' '.join(arr)\n",
        "    test_overview.append(noun_str)\n",
        "\n",
        "test_vectors = vectorizer.transform(test_overview)\n",
        "test_vectors = test_vectors.todense()\n",
        "\n",
        "test_dataset = CustomDataset(test_df['img_path'].values, test_vectors, None, test_transform, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d1d0373",
      "metadata": {},
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdc3618a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Averaged train CV Accuracy: 0.88 (+/- 0.00)\n",
            "Averaged val CV Accuracy: 0.73 (+/- 0.00)\n",
            "[118 121 118 ...  73 118  44]\n",
            "(7280,)\n"
          ]
        }
      ],
      "source": [
        "# Make prediction \n",
        "result = BOW_XGB.predict(test_dataset.text_vectors)\n",
        "\n",
        "submit = pd.read_csv('./sample_submission.csv')\n",
        "submit['cat3'] = le.inverse_transform(result)\n",
        "submit.to_csv('./submit_xgb.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bUGqdi96YqcH",
      "metadata": {
        "id": "bUGqdi96YqcH"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i57J7xBqYqcH",
      "metadata": {
        "id": "i57J7xBqYqcH"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv('./test.csv')\n",
        "test_vectors = vectorizer.transform(test_df['overview'])\n",
        "test_vectors = test_vectors.todense()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0H65ivkQYqcI",
      "metadata": {
        "id": "0H65ivkQYqcI"
      },
      "outputs": [],
      "source": [
        "test_dataset = CustomDataset(test_df['img_path'].values, test_vectors, None, test_transform, True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6JL-kbgiYqcI",
      "metadata": {
        "id": "6JL-kbgiYqcI"
      },
      "outputs": [],
      "source": [
        "def inference(model, test_loader, deivce):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    model_preds = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for img, text in tqdm(iter(test_loader)):\n",
        "            img = img.float().to(device)\n",
        "            text = text.to(device)\n",
        "            \n",
        "            model_pred = model(img, text)\n",
        "            model_preds += model_pred.argmax(1).detach().cpu().numpy().tolist()\n",
        "    # img, text에 따른 예측값들을 model_preds 배열에 넣어 리턴\n",
        "    return model_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "um1Bl-Z1YqcI",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "81402dbf8e164136b11886d994c3fc31"
          ]
        },
        "id": "um1Bl-Z1YqcI",
        "outputId": "bedeff43-5e71-4756-dfa6-67d79be1a41e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 114/114 [01:45<00:00,  1.08it/s]\n"
          ]
        }
      ],
      "source": [
        "preds = inference(infer_model, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LVAdfN8AYqcI",
      "metadata": {
        "id": "LVAdfN8AYqcI"
      },
      "source": [
        "## Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iO9HW8TjYqcI",
      "metadata": {
        "id": "iO9HW8TjYqcI"
      },
      "outputs": [],
      "source": [
        "submit = pd.read_csv('./sample_submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fSvSVjEXYqcI",
      "metadata": {
        "id": "fSvSVjEXYqcI"
      },
      "outputs": [],
      "source": [
        "submit['cat3'] = le.inverse_transform(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rfLyrJ1uYqcI",
      "metadata": {
        "id": "rfLyrJ1uYqcI"
      },
      "outputs": [],
      "source": [
        "submit.to_csv('./submit_jgw.csv', index=False)\n",
        "# 제출 파일로 저장"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.0 ('tour2')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "fc20420bb98500be9b4895145e31801960d6347914f27859a9e38443081d250a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
