{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8mEilfefYqb4",
      "metadata": {
        "id": "8mEilfefYqb4"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "qPTWSFu-Yqb6",
      "metadata": {
        "id": "qPTWSFu-Yqb6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import albumentations as A # fast image agumentation library\n",
        "from albumentations.pytorch.transforms import ToTensorV2 # 이미지 형 변환\n",
        "#import torchvision.models as models\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "from transformers import BertTokenizerFast, BertModel, DistilBertModel\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e83efdc4",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "NjkSKcj3Yqb8",
      "metadata": {
        "id": "NjkSKcj3Yqb8"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# gpu 사용하기 위한 코드\n",
        "# cuda가 설치되어 있으면 gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IKfY4MLYYqb8",
      "metadata": {
        "id": "IKfY4MLYYqb8"
      },
      "source": [
        "## Hyperparameter Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "Ag8ewGK5Yqb9",
      "metadata": {
        "id": "Ag8ewGK5Yqb9"
      },
      "outputs": [],
      "source": [
        "CFG = {\n",
        "    'IMG_SIZE':224,\n",
        "    'EPOCHS':8,\n",
        "    'LEARNING_RATE':3e-4,\n",
        "    'BATCH_SIZE':1,\n",
        "    'SEED':41\n",
        "}\n",
        "# 이미지 사이즈, 이폭, 학습률, 배치사이즈, 시드 고정"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NHVDirSbYqb-",
      "metadata": {
        "id": "NHVDirSbYqb-"
      },
      "source": [
        "## Fixed RandomSeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "uDCxH4oxYqb-",
      "metadata": {
        "id": "uDCxH4oxYqb-"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(CFG['SEED']) # Seed 고정"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o7K4p5NkYqb_",
      "metadata": {
        "id": "o7K4p5NkYqb_"
      },
      "source": [
        "## Data Load & Train/Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "OQyg4_lJYqcA",
      "metadata": {
        "id": "OQyg4_lJYqcA"
      },
      "outputs": [],
      "source": [
        "all_df = pd.read_csv('./train_bert.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "EcTB0OXYYqcB",
      "metadata": {
        "id": "EcTB0OXYYqcB"
      },
      "outputs": [],
      "source": [
        "train_df, val_df, _, _ = train_test_split(all_df, all_df['cat3'], test_size=0.2, random_state=CFG['SEED'])\n",
        "# train set, validation set 구별"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3SepiZ4ZYqcB",
      "metadata": {
        "id": "3SepiZ4ZYqcB"
      },
      "source": [
        "## Label-Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "D7cJtPy8YqcB",
      "metadata": {
        "id": "D7cJtPy8YqcB",
        "outputId": "bcfb2c88-b28a-4083-b90d-6421238dc70b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(train_df['cat3'].values)\n",
        "# 카테고리형 데이터를 수치형으로 변환하는 labelencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "DWmb25r_YqcC",
      "metadata": {
        "id": "DWmb25r_YqcC"
      },
      "outputs": [],
      "source": [
        "train_df['cat3'] = le.transform(train_df['cat3'].values)\n",
        "val_df['cat3'] = le.transform(val_df['cat3'].values)\n",
        "# cat3에 labelencoder를 적용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OkIwC6lBYqcD",
      "metadata": {
        "id": "OkIwC6lBYqcD"
      },
      "source": [
        "## Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "RTlinSBbYqcD",
      "metadata": {
        "id": "RTlinSBbYqcD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "FgdPdmIbYqcD",
      "metadata": {
        "id": "FgdPdmIbYqcD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(13588, 2, 1, 512)\n",
            "(3398, 2, 1, 512)\n"
          ]
        }
      ],
      "source": [
        "# vectorizer = CountVectorizer(max_features=4096)\n",
        "# # overview를 vectorize하는 vectorizer 선언, 최대 특성 수는 4096\n",
        "\n",
        "# train_vectors = vectorizer.fit_transform(train_df['overview'])\n",
        "# train_vectors = train_vectors.todense()\n",
        "\n",
        "# val_vectors = vectorizer.transform(val_df['overview'])\n",
        "# val_vectors = val_vectors.todense()\n",
        "\n",
        "# train_vectors = vectorizer.fit_transform(train_df['overview'])\n",
        "# train_vectors = train_vectors.todense()\n",
        "\n",
        "# val_vectors = vectorizer.transform(val_df['overview'])\n",
        "# val_vectors = val_vectors.todense()\n",
        "MAX_LEN = 512\n",
        "train_vectors = []\n",
        "len_list = []\n",
        "\n",
        "character_remove = [\"<br>\", \"<br />\", \"*\", \"※\", \"<strong>\", \"<strong/>\", \"-\",]\n",
        "\n",
        "for i, ov in enumerate(train_df['overview']):\n",
        "    for char_remove in character_remove:\n",
        "        ov = ov.replace(char_remove, \"\")\n",
        "    \n",
        "    ov = ov.split('.')\n",
        "    ov = '. [SEP]'.join(ov)\n",
        "    ov = ov.split('\\n')\n",
        "    ov = '. [SEP]'.join(ov)\n",
        "    ov = \"[CLS] \" + ov\n",
        "\n",
        "    tokenized_text = tokenizer.tokenize(ov)\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_text]\n",
        "    len_list.append(len(input_ids))\n",
        "\n",
        "    input_ids = pad_sequences([input_ids], maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    attention_masks = []\n",
        "\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    vec = np.array([\n",
        "        input_ids,\n",
        "        attention_masks,\n",
        "    ])\n",
        "\n",
        "    train_vectors.append(vec)\n",
        "\n",
        "train_vectors = np.array(train_vectors)\n",
        "print(train_vectors.shape)\n",
        "\n",
        "val_vectors = []\n",
        "for i, ov in enumerate(val_df['overview']):\n",
        "    for char_remove in character_remove:\n",
        "        ov = ov.replace(char_remove, \"\")\n",
        "    \n",
        "    ov = ov.split('.')\n",
        "    ov = '. [SEP]'.join(ov)\n",
        "    ov = ov.split('\\n')\n",
        "    ov = '. [SEP]'.join(ov)\n",
        "    ov = \"[CLS] \" + ov\n",
        "\n",
        "    tokenized_text = tokenizer.tokenize(ov)\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_text]\n",
        "    input_ids = pad_sequences([input_ids], maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    attention_masks = []\n",
        "\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    vec = np.array([\n",
        "        input_ids,\n",
        "        attention_masks,\n",
        "    ])\n",
        "\n",
        "    val_vectors.append(vec)\n",
        "\n",
        "val_vectors = np.array(val_vectors)\n",
        "print(val_vectors.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f55bff6c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([3.456e+03, 5.036e+03, 2.819e+03, 1.166e+03, 4.830e+02, 2.240e+02,\n",
              "        1.480e+02, 8.700e+01, 5.600e+01, 3.900e+01, 2.200e+01, 1.900e+01,\n",
              "        9.000e+00, 6.000e+00, 4.000e+00, 4.000e+00, 4.000e+00, 0.000e+00,\n",
              "        0.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
              "        1.000e+00, 0.000e+00, 0.000e+00, 2.000e+00, 0.000e+00, 2.000e+00]),\n",
              " array([   6.        ,  112.43333333,  218.86666667,  325.3       ,\n",
              "         431.73333333,  538.16666667,  644.6       ,  751.03333333,\n",
              "         857.46666667,  963.9       , 1070.33333333, 1176.76666667,\n",
              "        1283.2       , 1389.63333333, 1496.06666667, 1602.5       ,\n",
              "        1708.93333333, 1815.36666667, 1921.8       , 2028.23333333,\n",
              "        2134.66666667, 2241.1       , 2347.53333333, 2453.96666667,\n",
              "        2560.4       , 2666.83333333, 2773.26666667, 2879.7       ,\n",
              "        2986.13333333, 3092.56666667, 3199.        ]),\n",
              " <BarContainer object of 30 artists>)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGdCAYAAAAMm0nCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAnTUlEQVR4nO3dfXDU9YHH8U8e2CU87Ian7JISMJQWiDx4RA17Va5KjkBjT0ucEcshp6gDF5xClIe0Hj705sLgWIrKQ+/oGWdOinAjWkkBYzDhlAUlNfIkObHxggebUGl2ASEB8r0/nPzOlUQJJCTf8H7N7AzZ3/f32+/vOxvy5pfdJcYYYwQAAGCR2I6eAAAAQGsRMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsE9/RE2gvjY2NOnr0qHr37q2YmJiOng4AALgExhidPHlSycnJio1t+TpLlw2Yo0ePKiUlpaOnAQAALsORI0c0aNCgFrd32YDp3bu3pC8XwOPxdPBsAADApYhEIkpJSXF+jrekywZM06+NPB4PAQMAgGW+7eUfvIgXAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYp1UB8+STTyomJibqNmLECGf72bNnlZubq379+qlXr17KyclRTU1N1DGqq6uVnZ2tHj16KCkpSQsWLND58+ejxpSWlmrcuHFyu90aNmyYCgsLL/8MAQBAlxPf2h2uv/56vfXWW/9/gPj/P8T8+fNVVFSkjRs3yuv1au7cuZo6dareffddSdKFCxeUnZ0tv9+vnTt36tixY7rvvvvUrVs3/cu//IskqaqqStnZ2Zo9e7ZefvlllZSU6MEHH9TAgQOVlZV1pefb4a5bXHTZ+366NLsNZwIAgL1aHTDx8fHy+/0X3R8Oh/Xb3/5W69at0+233y5JevHFFzVy5Ejt2rVL48eP15tvvqmDBw/qrbfeks/n0w033KBf/vKXWrRokZ588km5XC6tWbNGqampevbZZyVJI0eO1DvvvKPly5d3iYABAABXrtWvgfn444+VnJysoUOHavr06aqurpYklZeX69y5c8rMzHTGjhgxQoMHD1YwGJQkBYNBjR49Wj6fzxmTlZWlSCSiAwcOOGO+eoymMU3HaEl9fb0ikUjUDQAAdE2tCpiMjAwVFhZq69atWr16taqqqnTrrbfq5MmTCoVCcrlcSkxMjNrH5/MpFApJkkKhUFS8NG1v2vZNYyKRiM6cOdPi3AoKCuT1ep1bSkpKa04NAABYpFW/QpoyZYrz5zFjxigjI0NDhgzRhg0blJCQ0OaTa438/Hzl5eU5X0ciESIGAIAu6oreRp2YmKjvf//7Onz4sPx+vxoaGlRXVxc1pqamxnnNjN/vv+hdSU1ff9sYj8fzjZHkdrvl8XiibgAAoGu6ooA5deqUPvnkEw0cOFDp6enq1q2bSkpKnO2VlZWqrq5WIBCQJAUCAe3bt0+1tbXOmOLiYnk8HqWlpTljvnqMpjFNxwAAAGhVwDz22GMqKyvTp59+qp07d+onP/mJ4uLidO+998rr9WrWrFnKy8vT22+/rfLyct1///0KBAIaP368JGnSpElKS0vTjBkz9OGHH2rbtm16/PHHlZubK7fbLUmaPXu2/vSnP2nhwoU6dOiQVq1apQ0bNmj+/Pltf/YAAMBKrXoNzGeffaZ7771Xn3/+uQYMGKBbbrlFu3bt0oABAyRJy5cvV2xsrHJyclRfX6+srCytWrXK2T8uLk6bN2/WnDlzFAgE1LNnT82cOVNPP/20MyY1NVVFRUWaP3++VqxYoUGDBmnt2rW8hRoAADhijDGmoyfRHiKRiLxer8LhcKd6PQwfZAcAQMsu9ec3/xcSAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA61xRwCxdulQxMTGaN2+ec9/Zs2eVm5urfv36qVevXsrJyVFNTU3UftXV1crOzlaPHj2UlJSkBQsW6Pz581FjSktLNW7cOLndbg0bNkyFhYVXMlUAANCFXHbAvP/++/rNb36jMWPGRN0/f/58vfHGG9q4caPKysp09OhRTZ061dl+4cIFZWdnq6GhQTt37tRLL72kwsJCLVmyxBlTVVWl7Oxs3XbbbaqoqNC8efP04IMPatu2bZc7XQAA0IVcVsCcOnVK06dP17/927+pT58+zv3hcFi//e1v9atf/Uq333670tPT9eKLL2rnzp3atWuXJOnNN9/UwYMH9R//8R+64YYbNGXKFP3yl7/UypUr1dDQIElas2aNUlNT9eyzz2rkyJGaO3eu7r77bi1fvrwNThkAANjusgImNzdX2dnZyszMjLq/vLxc586di7p/xIgRGjx4sILBoCQpGAxq9OjR8vl8zpisrCxFIhEdOHDAGfP1Y2dlZTnHaE59fb0ikUjUDQAAdE3xrd1h/fr1+uMf/6j333//om2hUEgul0uJiYlR9/t8PoVCIWfMV+OlaXvTtm8aE4lEdObMGSUkJFz02AUFBXrqqadaezoAAMBCrboCc+TIEf3sZz/Tyy+/rO7du7fXnC5Lfn6+wuGwczty5EhHTwkAALSTVgVMeXm5amtrNW7cOMXHxys+Pl5lZWV67rnnFB8fL5/Pp4aGBtXV1UXtV1NTI7/fL0ny+/0XvSup6etvG+PxeJq9+iJJbrdbHo8n6gYAALqmVgXMxIkTtW/fPlVUVDi3G2+8UdOnT3f+3K1bN5WUlDj7VFZWqrq6WoFAQJIUCAS0b98+1dbWOmOKi4vl8XiUlpbmjPnqMZrGNB0DAABc21r1GpjevXtr1KhRUff17NlT/fr1c+6fNWuW8vLy1LdvX3k8Hj3yyCMKBAIaP368JGnSpElKS0vTjBkztGzZMoVCIT3++OPKzc2V2+2WJM2ePVsvvPCCFi5cqAceeEDbt2/Xhg0bVFRU1BbnDAAALNfqF/F+m+XLlys2NlY5OTmqr69XVlaWVq1a5WyPi4vT5s2bNWfOHAUCAfXs2VMzZ87U008/7YxJTU1VUVGR5s+frxUrVmjQoEFau3atsrKy2nq6AADAQjHGGNPRk2gPkUhEXq9X4XC4U70e5rrFl38V6dOl2W04EwAAOp9L/fnN/4UEAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDrxHT0BG123uKijpwAAwDWNKzAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALBOqwJm9erVGjNmjDwejzwejwKBgLZs2eJsP3v2rHJzc9WvXz/16tVLOTk5qqmpiTpGdXW1srOz1aNHDyUlJWnBggU6f/581JjS0lKNGzdObrdbw4YNU2Fh4eWfIQAA6HJaFTCDBg3S0qVLVV5erj179uj222/XnXfeqQMHDkiS5s+frzfeeEMbN25UWVmZjh49qqlTpzr7X7hwQdnZ2WpoaNDOnTv10ksvqbCwUEuWLHHGVFVVKTs7W7fddpsqKio0b948Pfjgg9q2bVsbnTIAALBdjDHGXMkB+vbtq2eeeUZ33323BgwYoHXr1unuu++WJB06dEgjR45UMBjU+PHjtWXLFt1xxx06evSofD6fJGnNmjVatGiRjh8/LpfLpUWLFqmoqEj79+93HmPatGmqq6vT1q1bL3lekUhEXq9X4XBYHo/nSk7xItctLmrT412qT5dmd8jjAgBwtVzqz+/Lfg3MhQsXtH79ep0+fVqBQEDl5eU6d+6cMjMznTEjRozQ4MGDFQwGJUnBYFCjR4924kWSsrKyFIlEnKs4wWAw6hhNY5qOAQAAEN/aHfbt26dAIKCzZ8+qV69e2rRpk9LS0lRRUSGXy6XExMSo8T6fT6FQSJIUCoWi4qVpe9O2bxoTiUR05swZJSQkNDuv+vp61dfXO19HIpHWnhoAALBEq6/ADB8+XBUVFdq9e7fmzJmjmTNn6uDBg+0xt1YpKCiQ1+t1bikpKR09JQAA0E5aHTAul0vDhg1Tenq6CgoKNHbsWK1YsUJ+v18NDQ2qq6uLGl9TUyO/3y9J8vv9F70rqenrbxvj8XhavPoiSfn5+QqHw87tyJEjrT01AABgiSv+HJjGxkbV19crPT1d3bp1U0lJibOtsrJS1dXVCgQCkqRAIKB9+/aptrbWGVNcXCyPx6O0tDRnzFeP0TSm6Rgtcbvdztu7m24AAKBratVrYPLz8zVlyhQNHjxYJ0+e1Lp161RaWqpt27bJ6/Vq1qxZysvLU9++feXxePTII48oEAho/PjxkqRJkyYpLS1NM2bM0LJlyxQKhfT4448rNzdXbrdbkjR79my98MILWrhwoR544AFt375dGzZsUFFRx7zzBwAAdD6tCpja2lrdd999OnbsmLxer8aMGaNt27bpb//2byVJy5cvV2xsrHJyclRfX6+srCytWrXK2T8uLk6bN2/WnDlzFAgE1LNnT82cOVNPP/20MyY1NVVFRUWaP3++VqxYoUGDBmnt2rXKyspqo1MGAAC2u+LPgems+BwYAADs0+6fAwMAANBRCBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYJ1W/2eO6DhX8vZt3oINAOhKuAIDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALBOqwKmoKBAN910k3r37q2kpCTdddddqqysjBpz9uxZ5ebmql+/furVq5dycnJUU1MTNaa6ulrZ2dnq0aOHkpKStGDBAp0/fz5qTGlpqcaNGye3261hw4apsLDw8s4QAAB0Oa0KmLKyMuXm5mrXrl0qLi7WuXPnNGnSJJ0+fdoZM3/+fL3xxhvauHGjysrKdPToUU2dOtXZfuHCBWVnZ6uhoUE7d+7USy+9pMLCQi1ZssQZU1VVpezsbN12222qqKjQvHnz9OCDD2rbtm1tcMoAAMB2McYYc7k7Hz9+XElJSSorK9OECRMUDoc1YMAArVu3Tnfffbck6dChQxo5cqSCwaDGjx+vLVu26I477tDRo0fl8/kkSWvWrNGiRYt0/PhxuVwuLVq0SEVFRdq/f7/zWNOmTVNdXZ22bt16SXOLRCLyer0Kh8PyeDyXe4rNum5xUZse72r4dGl2R08BAIBvdak/v6/oNTDhcFiS1LdvX0lSeXm5zp07p8zMTGfMiBEjNHjwYAWDQUlSMBjU6NGjnXiRpKysLEUiER04cMAZ89VjNI1pOkZz6uvrFYlEom4AAKBruuyAaWxs1Lx58/SDH/xAo0aNkiSFQiG5XC4lJiZGjfX5fAqFQs6Yr8ZL0/ambd80JhKJ6MyZM83Op6CgQF6v17mlpKRc7qkBAIBO7rIDJjc3V/v379f69evbcj6XLT8/X+Fw2LkdOXKko6cEAADaSfzl7DR37lxt3rxZO3bs0KBBg5z7/X6/GhoaVFdXF3UVpqamRn6/3xnz3nvvRR2v6V1KXx3z9Xcu1dTUyOPxKCEhodk5ud1uud3uyzkdAABgmVZdgTHGaO7cudq0aZO2b9+u1NTUqO3p6enq1q2bSkpKnPsqKytVXV2tQCAgSQoEAtq3b59qa2udMcXFxfJ4PEpLS3PGfPUYTWOajgEAAK5trboCk5ubq3Xr1un1119X7969ndeseL1eJSQkyOv1atasWcrLy1Pfvn3l8Xj0yCOPKBAIaPz48ZKkSZMmKS0tTTNmzNCyZcsUCoX0+OOPKzc317mCMnv2bL3wwgtauHChHnjgAW3fvl0bNmxQUZF97/4BAABtr1VXYFavXq1wOKwf/vCHGjhwoHN75ZVXnDHLly/XHXfcoZycHE2YMEF+v1+vvvqqsz0uLk6bN29WXFycAoGA/v7v/1733Xefnn76aWdMamqqioqKVFxcrLFjx+rZZ5/V2rVrlZWV1QanDAAAbHdFnwPTmfE5MNH4HBgAgA2uyufAAAAAdAQCBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdeI7egK4Oq5bXHRF+3+6NLuNZgIAwJXjCgwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArNPqgNmxY4d+/OMfKzk5WTExMXrttdeithtjtGTJEg0cOFAJCQnKzMzUxx9/HDXmxIkTmj59ujwejxITEzVr1iydOnUqaszevXt16623qnv37kpJSdGyZctaf3YAAKBLanXAnD59WmPHjtXKlSub3b5s2TI999xzWrNmjXbv3q2ePXsqKytLZ8+edcZMnz5dBw4cUHFxsTZv3qwdO3bo4YcfdrZHIhFNmjRJQ4YMUXl5uZ555hk9+eST+td//dfLOEUAANDVxBhjzGXvHBOjTZs26a677pL05dWX5ORkPfroo3rsscckSeFwWD6fT4WFhZo2bZo++ugjpaWl6f3339eNN94oSdq6dat+9KMf6bPPPlNycrJWr16tX/ziFwqFQnK5XJKkxYsX67XXXtOhQ4cuaW6RSERer1fhcFgej+dyT7FZV/qhcDbig+wAAFfDpf78btPXwFRVVSkUCikzM9O5z+v1KiMjQ8FgUJIUDAaVmJjoxIskZWZmKjY2Vrt373bGTJgwwYkXScrKylJlZaX+8pe/NPvY9fX1ikQiUTcAANA1tWnAhEIhSZLP54u63+fzOdtCoZCSkpKitsfHx6tv375RY5o7xlcf4+sKCgrk9XqdW0pKypWfEAAA6JS6zLuQ8vPzFQ6HnduRI0c6ekoAAKCdtGnA+P1+SVJNTU3U/TU1Nc42v9+v2traqO3nz5/XiRMnosY0d4yvPsbXud1ueTyeqBsAAOia2jRgUlNT5ff7VVJS4twXiUS0e/duBQIBSVIgEFBdXZ3Ky8udMdu3b1djY6MyMjKcMTt27NC5c+ecMcXFxRo+fLj69OnTllMGAAAWanXAnDp1ShUVFaqoqJD05Qt3KyoqVF1drZiYGM2bN0///M//rN///vfat2+f7rvvPiUnJzvvVBo5cqQmT56shx56SO+9957effddzZ07V9OmTVNycrIk6ac//alcLpdmzZqlAwcO6JVXXtGKFSuUl5fXZicOAADsFd/aHfbs2aPbbrvN+bopKmbOnKnCwkItXLhQp0+f1sMPP6y6ujrdcsst2rp1q7p37+7s8/LLL2vu3LmaOHGiYmNjlZOTo+eee87Z7vV69eabbyo3N1fp6enq37+/lixZEvVZMQAA4Np1RZ8D05nxOTBti8+BAQBcDR3yOTAAAABXAwEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKwT39ETgB2uW1x02ft+ujS7DWcCAABXYAAAgIUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdeI7egLo+q5bXHTZ+366NLsNZwIA6Cq4AgMAAKxDwAAAAOsQMAAAwDoEDAAAsA4v4kWnxguAAQDN4QoMAACwDgEDAACsQ8AAAADr8BoYdFm8fgYAui6uwAAAAOt06iswK1eu1DPPPKNQKKSxY8fq+eef180339zR08I1gKs3ANC5ddorMK+88ory8vL0xBNP6I9//KPGjh2rrKws1dbWdvTUAABAB4sxxpiOnkRzMjIydNNNN+mFF16QJDU2NiolJUWPPPKIFi9e/K37RyIReb1ehcNheTyeNp3blfzrHPgmXL0BcK271J/fnfJXSA0NDSovL1d+fr5zX2xsrDIzMxUMBpvdp76+XvX19c7X4XBY0pcL0dYa679o82MCkjR4/sYOedz9T2V1yOMCwNc1/dz+tusrnTJg/vznP+vChQvy+XxR9/t8Ph06dKjZfQoKCvTUU09ddH9KSkq7zBHoSry/7ugZAEC0kydPyuv1tri9UwbM5cjPz1deXp7zdWNjo06cOKF+/fopJiamzR4nEokoJSVFR44cafNfTdmMdWkZa9M81qVlrE3LWJvmdaV1Mcbo5MmTSk5O/sZxnTJg+vfvr7i4ONXU1ETdX1NTI7/f3+w+brdbbrc76r7ExMT2mqI8Ho/1T5L2wLq0jLVpHuvSMtamZaxN87rKunzTlZcmnfJdSC6XS+np6SopKXHua2xsVElJiQKBQAfODAAAdAad8gqMJOXl5WnmzJm68cYbdfPNN+vXv/61Tp8+rfvvv7+jpwYAADpYpw2Ye+65R8ePH9eSJUsUCoV0ww03aOvWrRe9sPdqc7vdeuKJJy76ddW1jnVpGWvTPNalZaxNy1ib5l2L69JpPwcGAACgJZ3yNTAAAADfhIABAADWIWAAAIB1CBgAAGAdAqYVVq5cqeuuu07du3dXRkaG3nvvvY6eUrt68sknFRMTE3UbMWKEs/3s2bPKzc1Vv3791KtXL+Xk5Fz04YPV1dXKzs5Wjx49lJSUpAULFuj8+fNX+1Su2I4dO/TjH/9YycnJiomJ0WuvvRa13RijJUuWaODAgUpISFBmZqY+/vjjqDEnTpzQ9OnT5fF4lJiYqFmzZunUqVNRY/bu3atbb71V3bt3V0pKipYtW9bep3ZFvm1d/uEf/uGi59DkyZOjxnTFdSkoKNBNN92k3r17KykpSXfddZcqKyujxrTV909paanGjRsnt9utYcOGqbCwsL1P74pcytr88Ic/vOh5M3v27KgxXXFtVq9erTFjxjgfRhcIBLRlyxZn+7X6nGmRwSVZv369cblc5t///d/NgQMHzEMPPWQSExNNTU1NR0+t3TzxxBPm+uuvN8eOHXNux48fd7bPnj3bpKSkmJKSErNnzx4zfvx489d//dfO9vPnz5tRo0aZzMxM88EHH5g//OEPpn///iY/P78jTueK/OEPfzC/+MUvzKuvvmokmU2bNkVtX7p0qfF6vea1114zH374ofm7v/s7k5qaas6cOeOMmTx5shk7dqzZtWuX+a//+i8zbNgwc++99zrbw+Gw8fl8Zvr06Wb//v3md7/7nUlISDC/+c1vrtZpttq3rcvMmTPN5MmTo55DJ06ciBrTFdclKyvLvPjii2b//v2moqLC/OhHPzKDBw82p06dcsa0xffPn/70J9OjRw+Tl5dnDh48aJ5//nkTFxdntm7delXPtzUuZW3+5m/+xjz00ENRz5twOOxs76pr8/vf/94UFRWZ//7v/zaVlZXm5z//uenWrZvZv3+/Mebafc60hIC5RDfffLPJzc11vr5w4YJJTk42BQUFHTir9vXEE0+YsWPHNrutrq7OdOvWzWzcuNG576OPPjKSTDAYNMZ8+cMtNjbWhEIhZ8zq1auNx+Mx9fX17Tr39vT1H9SNjY3G7/ebZ555xrmvrq7OuN1u87vf/c4YY8zBgweNJPP+++87Y7Zs2WJiYmLM//7v/xpjjFm1apXp06dP1NosWrTIDB8+vJ3PqG20FDB33nlni/tcC+tijDG1tbVGkikrKzPGtN33z8KFC831118f9Vj33HOPycrKau9TajNfXxtjvgyYn/3sZy3uc62sjTHG9OnTx6xdu5bnTDP4FdIlaGhoUHl5uTIzM537YmNjlZmZqWAw2IEza38ff/yxkpOTNXToUE2fPl3V1dWSpPLycp07dy5qTUaMGKHBgwc7axIMBjV69OioDx/MyspSJBLRgQMHru6JtKOqqiqFQqGotfB6vcrIyIhai8TERN14443OmMzMTMXGxmr37t3OmAkTJsjlcjljsrKyVFlZqb/85S9X6WzaXmlpqZKSkjR8+HDNmTNHn3/+ubPtWlmXcDgsSerbt6+ktvv+CQaDUcdoGmPT30tfX5smL7/8svr3769Ro0YpPz9fX3zxhbPtWlibCxcuaP369Tp9+rQCgQDPmWZ02k/i7Uz+/Oc/68KFCxd9CrDP59OhQ4c6aFbtLyMjQ4WFhRo+fLiOHTump556Srfeeqv279+vUCgkl8t10X+Y6fP5FAqFJEmhUKjZNWva1lU0nUtz5/rVtUhKSoraHh8fr759+0aNSU1NvegYTdv69OnTLvNvT5MnT9bUqVOVmpqqTz75RD//+c81ZcoUBYNBxcXFXRPr0tjYqHnz5ukHP/iBRo0aJUlt9v3T0phIJKIzZ84oISGhPU6pzTS3NpL005/+VEOGDFFycrL27t2rRYsWqbKyUq+++qqkrr02+/btUyAQ0NmzZ9WrVy9t2rRJaWlpqqio4DnzNQQMWjRlyhTnz2PGjFFGRoaGDBmiDRs2WPUkR8eZNm2a8+fRo0drzJgx+u53v6vS0lJNnDixA2d29eTm5mr//v165513OnoqnU5La/Pwww87fx49erQGDhyoiRMn6pNPPtF3v/vdqz3Nq2r48OGqqKhQOBzWf/7nf2rmzJkqKyvr6Gl1SvwK6RL0799fcXFxF73au6amRn6/v4NmdfUlJibq+9//vg4fPiy/36+GhgbV1dVFjfnqmvj9/mbXrGlbV9F0Lt/0/PD7/aqtrY3afv78eZ04ceKaWq+hQ4eqf//+Onz4sKSuvy5z587V5s2b9fbbb2vQoEHO/W31/dPSGI/H0+n/kdHS2jQnIyNDkqKeN111bVwul4YNG6b09HQVFBRo7NixWrFiBc+ZZhAwl8Dlcik9PV0lJSXOfY2NjSopKVEgEOjAmV1dp06d0ieffKKBAwcqPT1d3bp1i1qTyspKVVdXO2sSCAS0b9++qB9QxcXF8ng8SktLu+rzby+pqany+/1RaxGJRLR79+6otairq1N5ebkzZvv27WpsbHT+cg4EAtqxY4fOnTvnjCkuLtbw4cM7/a9JLtVnn32mzz//XAMHDpTUddfFGKO5c+dq06ZN2r59+0W/Amur759AIBB1jKYxnfnvpW9bm+ZUVFRIUtTzpiuuTXMaGxtVX19/TT9nWtTRryK2xfr1643b7TaFhYXm4MGD5uGHHzaJiYlRr/buah599FFTWlpqqqqqzLvvvmsyMzNN//79TW1trTHmy7f0DR482Gzfvt3s2bPHBAIBEwgEnP2b3tI3adIkU1FRYbZu3WoGDBhg5duoT548aT744APzwQcfGEnmV7/6lfnggw/M//zP/xhjvnwbdWJionn99dfN3r17zZ133tns26j/6q/+yuzevdu888475nvf+17U24Xr6uqMz+czM2bMMPv37zfr1683PXr06NRvF/6mdTl58qR57LHHTDAYNFVVVeatt94y48aNM9/73vfM2bNnnWN0xXWZM2eO8Xq9prS0NOqtwF988YUzpi2+f5reErtgwQLz0UcfmZUrV3b6t8R+29ocPnzYPP3002bPnj2mqqrKvP7662bo0KFmwoQJzjG66tosXrzYlJWVmaqqKrN3716zePFiExMTY958801jzLX7nGkJAdMKzz//vBk8eLBxuVzm5ptvNrt27eroKbWre+65xwwcONC4XC7zne98x9xzzz3m8OHDzvYzZ86Yf/zHfzR9+vQxPXr0MD/5yU/MsWPHoo7x6aefmilTppiEhATTv39/8+ijj5pz585d7VO5Ym+//baRdNFt5syZxpgv30r9T//0T8bn8xm3220mTpxoKisro47x+eefm3vvvdf06tXLeDwec//995uTJ09Gjfnwww/NLbfcYtxut/nOd75jli5derVO8bJ807p88cUXZtKkSWbAgAGmW7duZsiQIeahhx66KPq74ro0tyaSzIsvvuiMaavvn7ffftvccMMNxuVymaFDh0Y9Rmf0bWtTXV1tJkyYYPr27WvcbrcZNmyYWbBgQdTnwBjTNdfmgQceMEOGDDEul8sMGDDATJw40YkXY67d50xLYowx5upd7wEAALhyvAYGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgnf8DNkWTOMAV3IwAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.hist(len_list, bins=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "108d56ae",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "40c3607d",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "P3xNr-gCYqcE",
      "metadata": {
        "id": "P3xNr-gCYqcE"
      },
      "source": [
        "## CustomDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "gWjyLPMDYqcE",
      "metadata": {
        "id": "gWjyLPMDYqcE"
      },
      "outputs": [],
      "source": [
        "# Dataset 생성\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, img_path_list, text_vectors, label_list, transforms, infer=False):\n",
        "        self.img_path_list = img_path_list\n",
        "        self.text_vectors = text_vectors\n",
        "        self.label_list = label_list\n",
        "        self.transforms = transforms\n",
        "        self.infer = infer\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # NL\n",
        "        token_vec = self.text_vectors[index]\n",
        "\n",
        "        # Image 읽기\n",
        "        img_path = self.img_path_list[index]\n",
        "        #image = Image.open(img_path)\n",
        "        #image = cv2.imread(img_path)\n",
        "        \n",
        "        # if self.transforms is not None:\n",
        "        #     image = self.transforms(image=image)['image'] # transforms(=image augmentation) 적용\n",
        "\n",
        "         # Label\n",
        "        if self.infer: # infer == True, test_data로부터 label \"결과 추출\" 시 사용\n",
        "            return torch.tensor(1), token_vec\n",
        "        else: # infer == False\n",
        "            label = self.label_list[index] # dataframe에서 label 가져와 \"학습\" 시 사용\n",
        "            return torch.tensor(1), token_vec, label\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_path_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "JrdLk7BvYqcF",
      "metadata": {
        "id": "JrdLk7BvYqcF"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "                            A.Resize(CFG['IMG_SIZE'],CFG['IMG_SIZE']),\n",
        "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
        "                            ToTensorV2()\n",
        "                            ])\n",
        "\n",
        "test_transform = A.Compose([\n",
        "                            A.Resize(CFG['IMG_SIZE'],CFG['IMG_SIZE']),\n",
        "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
        "                            ToTensorV2()\n",
        "                            ])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W6mw3pJXYqcF",
      "metadata": {
        "id": "W6mw3pJXYqcF"
      },
      "source": [
        "- albumentations -> fast image augmentation library\n",
        "\n",
        "- albumentations.Compose -> transform = A.Compose([])을 이용하여 이미지와 라벨 각각에 Augmentation을 적용하기 위한 객체를 생성\n",
        "\n",
        "- albumentations.Resize(128, 128) -> 128*128 size로 resize\n",
        "- albumentations.Normalize() -> 입력 받은 이미지 값의 범위를 (0, 255) → (-1, 1) 범위로 줄여주는 역할, 위에서는 평균값, 분산값, 최대 픽셀값으로 img = (img - mean * max_pixel_value) / (std * max_pixel_value)을 계산.\n",
        "- ToTensorV2 -> tensor형 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "28lLznspYqcF",
      "metadata": {
        "id": "28lLznspYqcF"
      },
      "outputs": [],
      "source": [
        "# __init__(self, img_path_list, text_vectors, label_list, transforms, infer=False)\n",
        "train_dataset = CustomDataset(train_df['img_path'].values, train_vectors, train_df['cat3'].values, train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0) # 6\n",
        "\n",
        "val_dataset = CustomDataset(val_df['img_path'].values, val_vectors, val_df['cat3'].values, test_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0) # 6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AjwCGRhnYqcG",
      "metadata": {
        "id": "AjwCGRhnYqcG"
      },
      "source": [
        "- DataLoader: Dataset와 Sampler를 결합하고 지정된 데이터 세트에 대해 반복 가능한 기능을 제공.    \n",
        "    - dataset (Dataset): 데이터를 로드할 데이터 집합.   \n",
        "    - batch_size (int, optional): **how many samples** per batch to load (default: ``1``).   \n",
        "    - num_workers (int, optional): **how many subprocesses** to use for data loading. ``0`` means that the data will be    loaded in the main process. (default: ``0``) -> 6으로 설정 시 안돌아감([Errno 32] Broken pipe). 0으로 변경해야 됨"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OOBrsD0nYqcG",
      "metadata": {
        "id": "OOBrsD0nYqcG"
      },
      "source": [
        "## Model Define"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "O_hoH59gYqcG",
      "metadata": {
        "id": "O_hoH59gYqcG"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, num_classes=len(le.classes_)):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=128)\n",
        "        self.model.cuda()\n",
        "\n",
        "        # Classifier\n",
        "        # self.classifier = nn.Sequential(\n",
        "        #     nn.AdaptiveAvgPool1d(1),\n",
        "        #     nn.Flatten(),\n",
        "        #     nn.Linear(768, 128),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Linear(128, num_classes),\n",
        "        # )\n",
        "\n",
        "    def forward(self, img, text):\n",
        "        # x = self.model(\n",
        "        #     text[:,0,0,:512], \n",
        "        #     token_type_ids=None, \n",
        "        #     attention_mask=[:,1,0,:512], \n",
        "        # )\n",
        "\n",
        "        \n",
        "        # x = self.model_bert(\n",
        "        #     input_ids=text[:,0,0,:512],\n",
        "        #     attention_mask=text[:,1,0,:512],\n",
        "        #     token_type_ids=text[:,2,0,:512],\n",
        "        # ).last_hidden_state\n",
        "        # x=x.transpose(1,2)\n",
        "        # print(x.shape)\n",
        "\n",
        "        # x = self.classifier(x)\n",
        "        \n",
        "        return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TajLAFGWYqcG",
      "metadata": {
        "id": "TajLAFGWYqcG"
      },
      "source": [
        "결론:\n",
        "- Image: conv -> ReLU -> MaxPooling -> conv -> relu -> maxpooling -> conv -> relu -> maxpooling -> conv -> relu -> maxpooling\n",
        "\n",
        "- Text: linear -> relu -> linear\n",
        "\n",
        "- classifier : linear"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9NWkAbI2YqcG",
      "metadata": {
        "id": "9NWkAbI2YqcG"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "Q4SIuzRRYqcG",
      "metadata": {
        "id": "Q4SIuzRRYqcG"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
        "    model.to(device) # gpu(cpu)에 적용\n",
        "    criterion = nn.CrossEntropyLoss().to(device) # CrossEntropyLoss: 다중분류를 위한 손실함수\n",
        "    best_score = 0\n",
        "    best_model = None # 최고의 모델을 추출하기 위한 파라미터\n",
        "    \n",
        "    for epoch in range(1,CFG[\"EPOCHS\"]+1):\n",
        "        model.train() # 학습시킴.\n",
        "        train_loss = []\n",
        "        correct = 0\n",
        "        for img, text, label in tqdm(iter(train_loader)): # train_loader에서 img, text, label 가져옴\n",
        "            img = img.float().to(device)\n",
        "            text = text.to(device).type(torch.LongTensor).to(device)\n",
        "            label = label.type(torch.LongTensor) # label type을 LongTensor로 형변환, 추가하여 에러 해결\n",
        "            label = label.to(device)\n",
        "            \n",
        "            optimizer.zero_grad() # 이전 루프에서 .grad에 저장된 값이 다음 루프의 업데이트에도 간섭하는 걸 방지, 0으로 초기화\n",
        "\n",
        "            model_pred = model(\n",
        "                text[:,0,0,:512], \n",
        "                token_type_ids=None, \n",
        "                attention_mask=text[:,1,0,:512], \n",
        "            ).logits # 예측\n",
        "            \n",
        "            loss = criterion(model_pred, label) # 예측값과 실제값과의 손실 계산\n",
        "\n",
        "            loss.backward() # .backward() 를 호출하면 역전파가 시작\n",
        "            optimizer.step() # optimizer.step()을 호출하여 역전파 단계에서 수집된 변화도로 매개변수를 조정\n",
        "\n",
        "            train_loss.append(loss.item())\n",
        "\n",
        "            pred_label = torch.argmax(model_pred, dim=1)\n",
        "            correct += (label == pred_label).float().sum()\n",
        "        \n",
        "        accuracy =  correct / float(len(train_loader.dataset))\n",
        "        # trainset, not train_loader\n",
        "        # probably x in your case\n",
        "\n",
        "        print(\"Train Accuracy = {}\".format(accuracy))\n",
        "            \n",
        "        # 모든 train_loss 가져옴\n",
        "        tr_loss = np.mean(train_loss)\n",
        "            \n",
        "        val_loss, val_score = validation(model, criterion, val_loader, device) # 검증 시작, 여기서 validation 함수 사용\n",
        "            \n",
        "        print(f'Epoch [{epoch}], Train Loss : [{tr_loss:.5f}] Val Loss : [{val_loss:.5f}] Val Score : [{val_score:.5f}]')\n",
        "        \n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "            # scheduler의 의미: Learning Rate Scheduler => learning rate를 조절한다. \n",
        "            # DACON에서는 CosineAnnealingLR 또는 CosineAnnealingWarmRestarts 를 주로 사용한다.\n",
        "            \n",
        "        if best_score < val_score: # 최고의 val_score을 가진 모델에 대해서만 최종적용을 시킴\n",
        "            best_score = val_score\n",
        "            best_model = model\n",
        "    \n",
        "    return best_model # val_score가 가장 높은 모델을 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ZDH3nVsTYqcH",
      "metadata": {
        "id": "ZDH3nVsTYqcH"
      },
      "outputs": [],
      "source": [
        "def score_function(real, pred):\n",
        "    return f1_score(real, pred, average=\"weighted\")\n",
        "\n",
        "def validation(model, criterion, val_loader, device):\n",
        "    model.eval() # nn.Module에서 train time과 eval time에서 수행하는 다른 작업을 수행할 수 있도록 switching 하는 함수\n",
        "    \n",
        "    model_preds = [] # 예측값\n",
        "    true_labels = [] # 실제값\n",
        "    \n",
        "    val_loss = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for img, text, label in tqdm(iter(val_loader)): # val_loader에서 img, text, label 가져옴\n",
        "            img = img.float().to(device)\n",
        "            text = text.to(device).type(torch.LongTensor).to(device)\n",
        "            label = label.type(torch.LongTensor) # label type을 LongTensor로 형변환, 추가하여 에러 해결\n",
        "            label = label.to(device)\n",
        "            \n",
        "            model_pred = model(\n",
        "                text[:,0,0,:512], \n",
        "                token_type_ids=None, \n",
        "                attention_mask=text[:,1,0,:512], \n",
        "            ).logits # 예측\n",
        "            #model_pred = model(img, text)\n",
        "            \n",
        "            loss = criterion(model_pred, label) # 예측값, 실제값으로 손실함수 적용 -> loss 추출\n",
        "            \n",
        "            val_loss.append(loss.item()) # loss 출력, val_loss에 저장\n",
        "            \n",
        "            model_preds += model_pred.argmax(1).detach().cpu().numpy().tolist()\n",
        "            true_labels += label.detach().cpu().numpy().tolist()\n",
        "        \n",
        "    test_weighted_f1 = score_function(true_labels, model_preds) # 실제 라벨값들과 예측한 라벨값들에 대해 f1 점수 계산\n",
        "    return np.mean(val_loss), test_weighted_f1 # 각각 val_loss, val_score에 적용됨"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gStGzRq_YqcH",
      "metadata": {
        "id": "gStGzRq_YqcH"
      },
      "source": [
        "## Run!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a9410c0f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  1%|          | 155/13588 [00:32<46:36,  4.80it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [20], line 16\u001b[0m\n\u001b[0;32m     10\u001b[0m total_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset) \u001b[38;5;241m*\u001b[39m CFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCHS\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     12\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m get_linear_schedule_with_warmup(optimizer, \n\u001b[0;32m     13\u001b[0m                                             num_warmup_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     14\u001b[0m                                             num_training_steps \u001b[38;5;241m=\u001b[39m total_steps)\n\u001b[1;32m---> 16\u001b[0m infer_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn [18], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_loader, val_loader, scheduler, device)\u001b[0m\n\u001b[0;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# .backward() 를 호출하면 역전파가 시작\u001b[39;00m\n\u001b[0;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# optimizer.step()을 호출하여 역전파 단계에서 수집된 변화도로 매개변수를 조정\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     32\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(model_pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     33\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (label \u001b[38;5;241m==\u001b[39m pred_label)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msum()\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#model = CustomModel()\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=128)\n",
        "\n",
        "#optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # 학습률\n",
        "                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n",
        "                )\n",
        "\n",
        "total_steps = len(train_loader.dataset) * CFG['EPOCHS']\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "infer_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f1ce547",
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "def xgb_bo(gamma,max_depth, subsample):\n",
        "    xgb_params = {\n",
        "        'gamma' : int(round(gamma)),\n",
        "        'max_depth' : int(round(max_depth)),\n",
        "        'subsample' : int(round(subsample)),      \n",
        "    }\n",
        "\n",
        "    xgb_clf = XGBClassifier(**xgb_params)\n",
        "    \n",
        "    xgb_clf.fit(train_dataset.text_vectors,train_dataset.label_list)\n",
        "\n",
        "    score = accuracy_score(val_dataset.label_list, xgb_clf.predict(val_dataset.text_vectors))\n",
        "\n",
        "    return score\n",
        "\n",
        "xgb_parameter_bounds = {\n",
        "    'gamma' : (0,10),\n",
        "    'max_depth' : (1,3), # 나무의 깊이\n",
        "    'subsample' : (0.5,1)\n",
        "}\n",
        "\n",
        "BO_xgb = BayesianOptimization(f = xgb_bo, pbounds = xgb_parameter_bounds,random_state = 0)\n",
        "\n",
        "# Bayesian Optimization을 실행해보세요\n",
        "\n",
        "BO_xgb.maximize(init_points = 5, n_iter = 5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a82ab535",
      "metadata": {},
      "source": [
        "# XGBoost Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g6x70sUqYqcH",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "970fb5ea76dc4b78a957370c1c81ee77",
            "9d09a89302a44259878e43b7d779ebdf",
            "6f8cd79f16864f3785f762e66dcd56fd",
            "2d7beb20eab844f595265748e4263d5f",
            "85c6c544b4584dc1bb75e92df40cc225",
            "3a9dec68ddb04929829a929806dab517",
            "916b5d6ead744b3a98a9d1e9081e4ec6",
            "ec5f883fc96b4671bceba18aecb8999f",
            "4f9055798b1d44e28dac4136e4f6bdef",
            "9b3a8d415c9848b6b773fcdc85315ab8"
          ]
        },
        "id": "g6x70sUqYqcH",
        "outputId": "a5c7dbde-af4a-485e-a665-d30d15740ded",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Averaged train CV Accuracy: 0.99 (+/- 0.00)\n",
            "Averaged val CV Accuracy: 0.56 (+/- 0.00)\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "#BOW_XGB = xgb.XGBClassifier(learning_rate=0.1, max_depth=3, n_estimators=100, objective=\"multi:softprob\", random_state=1, tree_method='gpu_hist', predictor='gpu_predictor')\n",
        "BOW_XGB = xgb.XGBClassifier(max_depth=3, n_estimators=100, objective=\"multi:softprob\")\n",
        "BOW_XGB.fit(train_dataset.text_vectors, train_dataset.label_list)\n",
        "\n",
        "\n",
        "train_score = BOW_XGB.score(train_dataset.text_vectors, train_dataset.label_list)\n",
        "val_score = BOW_XGB.score(val_dataset.text_vectors, val_dataset.label_list)\n",
        "print(\"Averaged train CV Accuracy: %0.2f (+/- %0.2f)\" % (train_score.mean(), train_score.std() * 2))\n",
        "print(\"Averaged val CV Accuracy: %0.2f (+/- %0.2f)\" % (val_score.mean(), val_score.std() * 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d77e984",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0.00776025, 0.00775048, 0.00774799, 0.0078957 , 0.00774468,\n",
              "        0.00775019, 0.00774485, 0.00774694, 0.00774736, 0.00774509,\n",
              "        0.00774565, 0.00790768, 0.00774506, 0.0077469 , 0.00774525,\n",
              "        0.00774572, 0.00777256, 0.00775631, 0.00775758, 0.00775535,\n",
              "        0.00776521, 0.00805745, 0.00774643, 0.00774767, 0.007745  ,\n",
              "        0.0077451 , 0.0077516 , 0.00774597, 0.00774909, 0.00774504,\n",
              "        0.00774586, 0.007753  , 0.00774495, 0.00774577, 0.00774477,\n",
              "        0.00774454, 0.00774637, 0.00774577, 0.00774695, 0.00778864,\n",
              "        0.00774546, 0.00787829, 0.0077458 , 0.00774451, 0.00775682,\n",
              "        0.00774643, 0.00774597, 0.00774812, 0.00774426, 0.00774803,\n",
              "        0.00774612, 0.00774564, 0.00779119, 0.00776246, 0.00775944,\n",
              "        0.00775311, 0.00774866, 0.00774756, 0.00778362, 0.0077457 ,\n",
              "        0.00774733, 0.0077507 , 0.00774624, 0.00774527, 0.00774849,\n",
              "        0.00774641, 0.00774565, 0.00774784, 0.00774593, 0.00774507,\n",
              "        0.00774595, 0.00774807, 0.00774763, 0.00782807, 0.00774652,\n",
              "        0.00774572, 0.00774538, 0.0077503 , 0.0077456 , 0.00775008,\n",
              "        0.00774545, 0.00779168, 0.00781779, 0.00774492, 0.00785617,\n",
              "        0.00777992, 0.00821959, 0.00774544, 0.00774575, 0.00775056,\n",
              "        0.00774569, 0.00774667, 0.00774922, 0.0081004 , 0.00774558,\n",
              "        0.0077458 , 0.00774596, 0.00784884, 0.00774541, 0.00781318,\n",
              "        0.007747  , 0.00774413, 0.00774593, 0.00774515, 0.00774683,\n",
              "        0.0077456 , 0.00774509, 0.00774562, 0.00775349, 0.00774402,\n",
              "        0.00775675, 0.00776898, 0.00782126, 0.00774449, 0.00774492,\n",
              "        0.00774913, 0.00776987, 0.00774552, 0.00960345, 0.0077475 ,\n",
              "        0.01180149, 0.00777276, 0.00774687, 0.00774593, 0.00774533,\n",
              "        0.00776236, 0.00774801, 0.00775243], dtype=float32),\n",
              " 21)"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def softmax(x):\n",
        "    max = np.max(x,axis=1,keepdims=True) #returns max of each row and keeps same dims\n",
        "    e_x = np.exp(x - max) #subtracts each row with its max value\n",
        "    sum = np.sum(e_x,axis=1,keepdims=True) #returns sum of each row and keeps same dims\n",
        "    f_x = e_x / sum \n",
        "    return f_x\n",
        "\n",
        "result = BOW_XGB.predict_proba(val_dataset.text_vectors)\n",
        "result = softmax(result)\n",
        "result[0], val_dataset.label_list[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72891765",
      "metadata": {},
      "source": [
        "# NB Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4218260e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(train_dataset.text_vectors, train_dataset.label_list)\n",
        "\n",
        "train_score = nb_classifier.score(train_dataset.text_vectors, train_dataset.label_list)\n",
        "val_score = nb_classifier.score(val_dataset.text_vectors, val_dataset.label_list)\n",
        "\n",
        "print(\"Averaged train CV Accuracy: %0.2f (+/- %0.2f)\" % (train_score.mean(), train_score.std() * 2))\n",
        "print(\"Averaged val CV Accuracy: %0.2f (+/- %0.2f)\" % (val_score.mean(), val_score.std() * 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d66ab13",
      "metadata": {},
      "source": [
        "# RandomForest Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4ffb528",
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [28], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m      3\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m train_score \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mscore(train_dataset\u001b[38;5;241m.\u001b[39mtext_vectors, train_dataset\u001b[38;5;241m.\u001b[39mlabel_list)\n\u001b[0;32m      7\u001b[0m val_score \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mscore(val_dataset\u001b[38;5;241m.\u001b[39mtext_vectors, val_dataset\u001b[38;5;241m.\u001b[39mlabel_list)\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:476\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    465\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    466\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    467\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    468\u001b[0m ]\n\u001b[0;32m    470\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 476\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    477\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    478\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    479\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    480\u001b[0m )(\n\u001b[0;32m    481\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    482\u001b[0m         t,\n\u001b[0;32m    483\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    484\u001b[0m         X,\n\u001b[0;32m    485\u001b[0m         y,\n\u001b[0;32m    486\u001b[0m         sample_weight,\n\u001b[0;32m    487\u001b[0m         i,\n\u001b[0;32m    488\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    489\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    490\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    491\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    492\u001b[0m     )\n\u001b[0;32m    493\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    494\u001b[0m )\n\u001b[0;32m    496\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:189\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    187\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 189\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    190\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\sklearn\\tree\\_classes.py:969\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    940\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    970\u001b[0m         X,\n\u001b[0;32m    971\u001b[0m         y,\n\u001b[0;32m    972\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    973\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    974\u001b[0m     )\n\u001b[0;32m    975\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\sklearn\\tree\\_classes.py:458\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    448\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    449\u001b[0m         splitter,\n\u001b[0;32m    450\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    456\u001b[0m     )\n\u001b[1;32m--> 458\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    461\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=5,random_state=0)\n",
        "rf.fit(train_dataset.text_vectors, train_dataset.label_list)\n",
        "\n",
        "train_score = rf.score(train_dataset.text_vectors, train_dataset.label_list)\n",
        "val_score = rf.score(val_dataset.text_vectors, val_dataset.label_list)\n",
        "\n",
        "print(\"Averaged train CV Accuracy: %0.2f (+/- %0.2f)\" % (train_score.mean(), train_score.std() * 2))\n",
        "print(\"Averaged val CV Accuracy: %0.2f (+/- %0.2f)\" % (val_score.mean(), val_score.std() * 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c588d8a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Averaged train CV Accuracy: 0.36 (+/- 0.00)\n",
            "Averaged val CV Accuracy: 0.35 (+/- 0.00)\n"
          ]
        }
      ],
      "source": [
        "train_score = rf.score(train_dataset.text_vectors, train_dataset.label_list)\n",
        "val_score = rf.score(val_dataset.text_vectors, val_dataset.label_list)\n",
        "\n",
        "print(\"Averaged train CV Accuracy: %0.2f (+/- %0.2f)\" % (train_score.mean(), train_score.std() * 2))\n",
        "print(\"Averaged val CV Accuracy: %0.2f (+/- %0.2f)\" % (val_score.mean(), val_score.std() * 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30d7f019",
      "metadata": {},
      "source": [
        "# Load Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cce9f02",
      "metadata": {},
      "outputs": [],
      "source": [
        "# load test dataset\n",
        "test_df = pd.read_csv('./test.csv')\n",
        "test_overview = []\n",
        "for overview in test_df['overview']:\n",
        "    arr = mecab.nouns(overview)\n",
        "    noun_str = ' '.join(arr)\n",
        "    test_overview.append(noun_str)\n",
        "\n",
        "test_vectors = vectorizer.transform(test_overview)\n",
        "test_vectors = test_vectors.todense()\n",
        "\n",
        "test_dataset = CustomDataset(test_df['img_path'].values, test_vectors, None, test_transform, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d1d0373",
      "metadata": {},
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdc3618a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Averaged train CV Accuracy: 0.88 (+/- 0.00)\n",
            "Averaged val CV Accuracy: 0.73 (+/- 0.00)\n",
            "[118 121 118 ...  73 118  44]\n",
            "(7280,)\n"
          ]
        }
      ],
      "source": [
        "# Make prediction \n",
        "result = BOW_XGB.predict(test_dataset.text_vectors)\n",
        "\n",
        "submit = pd.read_csv('./sample_submission.csv')\n",
        "submit['cat3'] = le.inverse_transform(result)\n",
        "submit.to_csv('./submit_xgb.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bUGqdi96YqcH",
      "metadata": {
        "id": "bUGqdi96YqcH"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i57J7xBqYqcH",
      "metadata": {
        "id": "i57J7xBqYqcH"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv('./test.csv')\n",
        "test_vectors = vectorizer.transform(test_df['overview'])\n",
        "test_vectors = test_vectors.todense()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0H65ivkQYqcI",
      "metadata": {
        "id": "0H65ivkQYqcI"
      },
      "outputs": [],
      "source": [
        "test_dataset = CustomDataset(test_df['img_path'].values, test_vectors, None, test_transform, True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6JL-kbgiYqcI",
      "metadata": {
        "id": "6JL-kbgiYqcI"
      },
      "outputs": [],
      "source": [
        "def inference(model, test_loader, deivce):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    model_preds = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for img, text in tqdm(iter(test_loader)):\n",
        "            img = img.float().to(device)\n",
        "            text = text.to(device)\n",
        "            \n",
        "            model_pred = model(img, text)\n",
        "            model_preds += model_pred.argmax(1).detach().cpu().numpy().tolist()\n",
        "    # img, text에 따른 예측값들을 model_preds 배열에 넣어 리턴\n",
        "    return model_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "um1Bl-Z1YqcI",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "81402dbf8e164136b11886d994c3fc31"
          ]
        },
        "id": "um1Bl-Z1YqcI",
        "outputId": "bedeff43-5e71-4756-dfa6-67d79be1a41e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 114/114 [01:45<00:00,  1.08it/s]\n"
          ]
        }
      ],
      "source": [
        "preds = inference(infer_model, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LVAdfN8AYqcI",
      "metadata": {
        "id": "LVAdfN8AYqcI"
      },
      "source": [
        "## Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iO9HW8TjYqcI",
      "metadata": {
        "id": "iO9HW8TjYqcI"
      },
      "outputs": [],
      "source": [
        "submit = pd.read_csv('./sample_submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fSvSVjEXYqcI",
      "metadata": {
        "id": "fSvSVjEXYqcI"
      },
      "outputs": [],
      "source": [
        "submit['cat3'] = le.inverse_transform(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rfLyrJ1uYqcI",
      "metadata": {
        "id": "rfLyrJ1uYqcI"
      },
      "outputs": [],
      "source": [
        "submit.to_csv('./submit_jgw.csv', index=False)\n",
        "# 제출 파일로 저장"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.0 ('tour2')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "fc20420bb98500be9b4895145e31801960d6347914f27859a9e38443081d250a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
