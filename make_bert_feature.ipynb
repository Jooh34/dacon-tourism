{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mnh51\\anaconda3\\envs\\tour2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': False, '_parameters': OrderedDict(), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_post_hooks': OrderedDict(), '_modules': OrderedDict([('embeddings', BertEmbeddings(\n",
      "  (word_embeddings): Embedding(42000, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 768)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")), ('encoder', BertEncoder(\n",
      "  (layer): ModuleList(\n",
      "    (0): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")), ('pooler', BertPooler(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (activation): Tanh()\n",
      "))]), 'config': BertConfig {\n",
      "  \"_name_or_path\": \"kykim/bert-kor-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"embedding_size\": 768,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.3.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 42000\n",
      "}\n",
      ", 'name_or_path': 'kykim/bert-kor-base'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertModel\n",
    "tokenizer_bert = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "model_bert = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n",
    "\n",
    "print(model_bert.train())\n",
    "print(model_bert.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 26])\n",
      "tensor([[    2, 16453,  2016,  5590, 18421,  2014, 23589, 14414,  8034, 14328,\n",
      "         13992,  2016, 14748, 19686,  2014, 33474,  8018, 21457, 14140, 17045,\n",
      "         26827, 13992,  2016,     0,     0,     3]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]])\n",
      "{'last_hidden_state': tensor([[[ 0.3753, -0.1631,  0.3628,  ...,  0.1089, -1.4743,  0.3935],\n",
      "         [ 0.4944,  0.2368, -0.2590,  ...,  0.1082, -1.4293, -0.0469],\n",
      "         [ 0.4803, -0.3952,  0.2008,  ...,  0.9918, -1.9435, -0.3293],\n",
      "         ...,\n",
      "         [ 1.6074,  0.6423,  0.0538,  ..., -0.3423, -0.9340, -1.1100],\n",
      "         [ 1.1927,  0.1356, -0.5316,  ..., -0.6882, -0.4671, -0.5094],\n",
      "         [ 1.3811,  0.1645,  0.3409,  ..., -0.1788, -1.7024, -0.7521]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), 'pooler_output': tensor([[ 0.3681,  0.0123, -0.9371,  0.9423, -0.9340,  0.3335, -0.4068, -0.8874,\n",
      "          0.9993,  0.5212, -0.6977,  0.5988,  0.4235, -0.7543,  0.6225,  0.9650,\n",
      "         -0.9968,  0.3325, -0.5252, -0.2936, -0.1128,  0.0747,  0.9976, -0.3993,\n",
      "         -0.4781,  0.1537, -0.5254, -0.4847, -0.0471,  0.9968, -0.8343, -0.9735,\n",
      "         -0.4733, -0.6755, -0.3138, -0.1286, -0.5782, -0.0659,  0.1336,  0.5617,\n",
      "         -0.9941,  0.3971, -0.1620, -0.2675,  0.3615, -0.4955,  0.1941, -0.7166,\n",
      "         -0.9851,  0.0074,  0.5349,  0.1231,  0.2468, -0.9941,  0.9998,  0.2961,\n",
      "          0.2837, -0.1646, -0.9996,  0.3483, -0.9935, -0.2641,  0.3276,  0.6638,\n",
      "         -0.6456,  0.9832, -0.0710, -0.6501, -0.9992,  0.1444, -0.9854, -0.3085,\n",
      "          0.5691, -0.2568, -0.7457, -0.1945,  0.6679, -0.9772, -0.3581, -0.9927,\n",
      "          0.9968, -0.9964,  0.1626, -0.7799,  0.9993, -0.3700,  0.2912,  0.3851,\n",
      "          0.9999,  0.9310,  0.8355, -0.2687,  0.1205,  0.0513, -0.2367,  0.2046,\n",
      "         -0.7210,  0.3459, -0.2155, -0.3607,  0.4835, -0.1854,  0.7470, -0.0830,\n",
      "          0.2963, -0.4340,  0.9169, -0.0083,  0.3639,  0.1147, -0.0876,  0.4402,\n",
      "         -0.9988, -0.4568, -0.1939,  0.2278,  0.4061, -0.2246, -0.5073,  0.4694,\n",
      "         -0.3645, -0.7331, -0.7524,  0.7606, -0.9929, -0.0158, -0.2001, -0.0883,\n",
      "         -0.5542,  0.1281,  0.9832,  0.5240, -0.4508,  0.0524,  0.3078,  0.8113,\n",
      "         -0.9491,  0.6146, -0.5872,  0.9682,  0.9887, -0.9747, -0.3469, -0.9990,\n",
      "          0.6565, -0.6139,  0.0294, -0.3597, -0.0438,  0.0586, -0.4200, -0.7385,\n",
      "          0.9799, -0.9999, -0.4076,  0.9878, -0.6436, -0.3490,  0.1339,  0.2914,\n",
      "         -0.3186, -0.3170, -0.3160, -0.5139, -0.5164,  0.2286, -0.2441, -0.0036,\n",
      "         -0.0324,  0.9914,  0.9993, -0.9813,  0.6957,  0.0143, -0.1824,  0.6614,\n",
      "          0.9986, -0.5305, -0.0195, -0.0158, -0.2182,  0.2880,  0.9102, -0.7975,\n",
      "          0.5040, -0.7463,  0.6118, -0.5385, -0.3670, -0.5941,  0.1765,  0.2292,\n",
      "          0.6035, -0.8415, -0.9999, -0.5554, -0.6034, -0.5688, -0.3648, -0.2905,\n",
      "         -0.2822,  0.0034,  0.0475,  0.9675,  0.0951, -0.0594,  0.9885, -0.2815,\n",
      "         -0.5107, -0.6356, -0.4700, -0.5755, -0.4732, -0.3244,  0.3443, -0.9428,\n",
      "          0.7010, -0.0332, -0.5363, -0.0328, -0.2249, -0.0422, -0.1820, -0.0254,\n",
      "          0.5096,  0.5909, -0.2807, -0.1437,  0.9849,  0.7539,  0.9996,  0.0361,\n",
      "         -0.0215, -0.0736,  0.1047,  0.2650,  0.9843, -0.8061,  0.9977,  0.7275,\n",
      "          0.0654,  0.8608,  0.0131,  0.1146, -0.0399,  0.0812,  0.1445, -0.8029,\n",
      "          0.9986, -0.9966, -0.4615,  0.5263, -0.1169, -0.2750,  0.1720, -0.5051,\n",
      "         -0.0059,  0.1961,  0.2681, -0.2326,  0.0359,  0.1468, -0.9968,  0.5300,\n",
      "          0.8335, -0.9796, -0.4156,  0.0511,  0.2152,  0.1407,  0.0631,  0.0402,\n",
      "          0.1626,  0.9953,  0.9844, -0.4995,  0.0231, -0.2245, -0.3637,  0.1956,\n",
      "         -0.6113, -0.6836, -0.4847, -0.6009, -0.2131, -0.0140,  0.6279, -0.3991,\n",
      "          0.1400, -0.8641,  0.9637,  0.1777, -0.5324,  0.2539,  0.1195,  0.0690,\n",
      "         -0.9989, -0.9973, -0.2795, -0.0652, -0.9983, -0.5884,  0.2857, -0.0528,\n",
      "          0.3725, -0.1736,  0.2128,  0.9986,  0.5853,  0.1577,  0.1515,  0.1856,\n",
      "         -0.6696,  0.1468,  0.4247,  0.9828, -0.1996,  0.3096, -0.9794,  0.6189,\n",
      "         -0.0594,  0.2004, -0.5699, -0.9457,  0.0147,  0.0533, -0.2646,  0.9969,\n",
      "          0.5253,  0.2163,  0.2281,  0.7180, -0.8530, -0.5032,  0.9785,  0.0463,\n",
      "          0.6627,  0.0962, -0.2798, -0.1808, -0.7858, -0.1258, -0.9758,  0.9929,\n",
      "         -0.6534, -0.2187,  0.4360, -0.4443, -0.1335, -0.5836,  0.5793, -0.9992,\n",
      "         -0.9336,  0.4967, -0.2041,  0.3752,  0.3047,  0.6080,  0.9132,  0.9058,\n",
      "         -0.4165, -0.3726,  0.0237, -0.6946,  0.5195, -0.0224, -0.8336, -0.9373,\n",
      "          0.0532,  0.3724, -0.9997, -0.1715, -0.5863,  0.0362, -0.2992, -0.2838,\n",
      "          0.0654,  0.4507, -0.5626,  0.9948, -0.0519, -0.1553, -0.9866, -0.0172,\n",
      "         -0.2453, -0.4761,  0.2216,  0.5775, -0.0264, -0.2729, -0.5720, -0.1840,\n",
      "          0.4367,  0.7733,  0.7171,  0.9862,  0.6137,  0.6945,  0.3062, -0.6425,\n",
      "         -0.9996, -0.9082, -0.6922,  0.9379, -0.3266,  0.8477,  0.5384, -0.9774,\n",
      "          0.9971,  0.3569,  0.4215, -0.2958,  0.7707, -0.6127, -0.8842, -0.5504,\n",
      "          0.9938,  0.0541, -0.5303,  0.9448,  0.6133,  0.4102,  0.1582,  0.1358,\n",
      "          0.9105, -0.7048,  0.8332,  0.2624,  0.3713, -0.4536,  0.0261,  0.9990,\n",
      "         -0.8059, -0.9839,  0.8594,  0.0218, -0.4954,  0.2479,  1.0000, -0.9128,\n",
      "         -0.1403,  0.1198,  0.1586,  0.6789, -0.9982, -0.4497,  0.1384, -0.5958,\n",
      "          0.4833, -0.9998, -0.1514, -0.2981, -0.0121,  0.5745,  0.9448, -0.5096,\n",
      "          0.5242,  0.5252,  0.9736,  0.9875,  0.5555,  0.5108,  0.3593,  0.2020,\n",
      "         -0.5480, -0.0498, -0.5657, -0.9328, -0.9415,  0.0991,  0.0534,  0.5277,\n",
      "         -0.3275, -0.9998,  0.1117,  0.4908,  0.5707,  0.7199, -0.9217, -0.9780,\n",
      "         -0.5988,  0.7499,  0.9971, -0.4425,  0.8810, -0.5001, -0.5501, -0.1960,\n",
      "          0.2363, -0.5982,  0.9762,  0.1648,  0.1872, -0.2383, -0.0426,  0.9594,\n",
      "          0.4611, -0.4792, -0.8425,  0.0490, -0.1111,  0.1777, -0.9999,  0.9915,\n",
      "          0.7274,  0.9996, -0.0139, -0.2615, -0.0597,  0.6888,  0.0388, -0.3936,\n",
      "          0.5837,  0.3206,  0.9885,  0.1918, -0.6714,  0.4724,  0.8855, -0.5332,\n",
      "          0.5138, -0.9870,  0.2956, -0.4656,  0.1540, -0.3198,  0.9516,  0.9418,\n",
      "          0.1591, -0.2789, -0.5919, -0.9383, -0.3221, -0.9431,  0.1661, -0.2278,\n",
      "         -0.2258, -0.2695, -0.4211,  0.6659,  0.6054, -0.6134, -0.4991,  0.9975,\n",
      "         -0.4141,  0.8437, -0.3943,  0.3569, -0.6361,  0.5576,  0.8482,  0.8138,\n",
      "         -0.0520, -0.0867, -0.3173,  0.1376, -0.4134,  0.1938,  0.8956, -0.8246,\n",
      "          0.6057, -0.3597, -0.1168, -0.6173,  0.2311,  0.5509, -0.3784, -0.1199,\n",
      "          0.5881, -0.9993, -0.2512,  0.6175,  0.3986,  0.9991,  0.5532,  0.9460,\n",
      "          0.2356,  0.5934,  0.9857, -0.1293,  0.0677, -0.1963, -0.0300, -0.6121,\n",
      "         -0.1917, -0.9993, -0.6497, -0.9987, -0.5836, -0.3561, -0.4243, -0.5739,\n",
      "          0.8342,  0.7793,  0.8514, -0.0623,  0.2711, -0.9995,  0.2326, -0.1411,\n",
      "          0.2952, -0.7892, -0.1362, -0.1479,  0.1957, -0.3229, -0.6300, -0.1222,\n",
      "          0.5996,  0.3048, -0.3129,  0.8750, -0.2326,  0.9993,  0.6605,  0.6894,\n",
      "          0.2866, -0.4173,  0.3698,  0.3179, -0.2363,  0.5119, -0.7593,  0.1535,\n",
      "         -0.6838, -0.9978,  0.9992,  0.2618,  0.8184,  0.0357,  0.9940, -0.8918,\n",
      "          0.0325,  0.1988, -0.3892,  0.4603,  0.1078,  0.9216, -0.1109,  0.5942,\n",
      "         -0.9995,  0.3550, -0.2826, -0.6128,  0.0460,  0.1226, -0.7357, -0.2690,\n",
      "          0.7007, -0.8566,  0.8055, -0.9999,  0.2295,  0.9682,  0.8207, -0.9037,\n",
      "         -0.2952, -0.9833,  0.9900,  0.6077, -0.2224,  0.3606,  0.5638, -0.4196,\n",
      "          0.9991,  0.3206,  0.5255,  0.3097, -0.3092,  0.9155, -0.0068, -0.9360,\n",
      "          0.6968,  0.0212, -0.4621,  0.9996,  0.9996,  0.9999,  0.0550, -0.5787,\n",
      "          0.4623, -0.9218, -0.7916,  0.9985,  0.9975,  0.6530, -0.6466,  0.0844,\n",
      "          0.3172, -0.1585, -0.3117,  0.6839, -0.1685,  0.9980,  0.6077,  0.8729,\n",
      "          0.0401, -0.9815,  0.0466, -0.5658, -0.0747,  0.4425,  0.2173, -0.0060,\n",
      "         -0.5639, -0.2433,  0.2111,  0.3826, -0.1226, -0.3072, -0.3573,  0.1698,\n",
      "         -0.6511,  0.7511,  0.0382, -0.6283, -0.9863, -0.1731, -0.7315, -0.9998,\n",
      "         -0.9968, -0.9320,  0.6171, -0.6133, -0.1836, -0.2763, -0.1147, -0.5162,\n",
      "          0.9289, -0.0842, -0.2869,  0.1688, -0.3231, -0.8835, -0.4622,  0.9104,\n",
      "          0.9925, -0.9996,  0.4555, -0.7555,  0.5567, -0.1541,  0.8333,  0.3593,\n",
      "         -0.0814, -0.0677,  0.4698, -0.0547, -0.2829, -0.8969, -0.2765,  0.4644,\n",
      "         -0.3704, -0.2156, -0.4575,  0.1649,  0.4406, -0.7867, -0.9806, -0.0553,\n",
      "         -0.4850, -0.0243,  0.1850,  0.8853,  0.4324,  0.0799,  0.5300,  0.4802]],\n",
      "       grad_fn=<TanhBackward0>), 'hidden_states': None, 'past_key_values': None, 'attentions': None, 'cross_attentions': None}\n",
      "torch.Size([1, 26, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer_bert(\"안녕하세요. 어 그러니까, 대한민국의 수도는 서울입니다. 정리하자면, 캠핑장이 위치해있는 곳은 가평입니다. [PAD][PAD]\", return_tensors='pt')\n",
    "print(inputs['input_ids'].shape)\n",
    "print(inputs['input_ids'])\n",
    "print(inputs['attention_mask'])\n",
    "print(inputs['token_type_ids'])\n",
    "output = model_bert(\n",
    "    input_ids=inputs['input_ids'][:,:512],\n",
    "    attention_mask=inputs['attention_mask'][:,:512],\n",
    "    token_type_ids=inputs['token_type_ids'][:,:512])\n",
    "print(output.__dict__)\n",
    "print(output.last_hidden_state.shape)\n",
    "print(output.pooler_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i :  1000\n",
      "i :  2000\n",
      "i :  3000\n",
      "i :  4000\n",
      "i :  5000\n",
      "i :  6000\n",
      "i :  7000\n",
      "i :  8000\n",
      "i :  9000\n",
      "i :  10000\n",
      "i :  11000\n",
      "i :  12000\n",
      "i :  13000\n",
      "i :  14000\n",
      "i :  15000\n",
      "i :  16000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "all_df = pd.read_csv('./train.csv')\n",
    "print(type(all_df))\n",
    "all_df['bert_emb'] = \"\"\n",
    "\n",
    "i=0\n",
    "for ov in all_df['overview']:\n",
    "    inputs = tokenizer_bert(ov, return_tensors='pt')\n",
    "    if len(inputs['input_ids'][0]) > 512:\n",
    "        output = model_bert(\n",
    "            input_ids=inputs['input_ids'][:,:512],\n",
    "            attention_mask=inputs['attention_mask'][:,:512],\n",
    "            token_type_ids=inputs['token_type_ids'][:,:512]\n",
    "        ).pooler_output\n",
    "    else:\n",
    "        output = model_bert(\n",
    "            input_ids=inputs['input_ids'][:,:512],\n",
    "            attention_mask=inputs['attention_mask'][:,:512],\n",
    "            token_type_ids=inputs['token_type_ids'][:,:512]\n",
    "        ).pooler_output\n",
    "\n",
    "    out = output.cpu().detach().numpy()\n",
    "    all_df['bert_emb'][i] = out[0]\n",
    "    i+=1\n",
    "\n",
    "    if i%1000 == 0:\n",
    "        print('i : ', i)\n",
    "\n",
    "all_df.to_csv('./train_bert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "tokenizer_bert = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "model_bert = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n",
    "\n",
    "all_df = pd.read_csv('./test.csv')\n",
    "print(type(all_df))\n",
    "all_df['bert_emb'] = \"\"\n",
    "\n",
    "i=0\n",
    "for ov in all_df['overview']:\n",
    "    inputs = tokenizer_bert(ov, return_tensors='pt')\n",
    "    if len(inputs['input_ids'][0]) > 512:\n",
    "        output = model_bert(\n",
    "            input_ids=inputs['input_ids'][:,:512],\n",
    "            attention_mask=inputs['attention_mask'][:,:512],\n",
    "            token_type_ids=inputs['token_type_ids'][:,:512]\n",
    "        ).pooler_output\n",
    "    else:\n",
    "        output = model_bert(\n",
    "            input_ids=inputs['input_ids'][:,:512],\n",
    "            attention_mask=inputs['attention_mask'][:,:512],\n",
    "            token_type_ids=inputs['token_type_ids'][:,:512]\n",
    "        ).pooler_output\n",
    "\n",
    "    out = output.cpu().detach().numpy()\n",
    "    all_df['bert_emb'][i] = out[0]\n",
    "    i+=1\n",
    "\n",
    "all_df.to_csv('./test_bert.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('tour2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc20420bb98500be9b4895145e31801960d6347914f27859a9e38443081d250a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
